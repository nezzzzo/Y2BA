{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7719b6-eee2-4cbb-b427-7fd6ed89abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix pyarrow/pandas compatibility issue\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fabaa8-e458-45b9-9e47-89c1543c97a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready.\n"
     ]
    }
   ],
   "source": [
    "# Core imports and paths\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust these paths as needed\n",
    "SEQ_PATH   = Path(\"artifacts/features/sequences/\")\n",
    "LABEL_PATH = Path(\"artifacts/labels/\")\n",
    "DENSE_PATH = Path(\"artifacts/features/dense/\")\n",
    "\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da2d8c3-c326-4264-9f34-dbd6c8580b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq shapes — train:(109816, 65), valid:(6102, 65), test:(6102, 65)\n",
      "Max seq length: 65\n"
     ]
    }
   ],
   "source": [
    "# Load tokenized sequences (npz with 'arr_0')\n",
    "X_train_seq = np.load(SEQ_PATH / \"X_train_seq_v1.npz\")[\"arr_0\"]\n",
    "X_valid_seq = np.load(SEQ_PATH / \"X_valid_seq_v1.npz\")[\"arr_0\"]\n",
    "X_test_seq  = np.load(SEQ_PATH / \"X_test_seq_v1.npz\")[\"arr_0\"]\n",
    "\n",
    "# Minimal sanity info\n",
    "print(f\"Seq shapes — train:{X_train_seq.shape}, valid:{X_valid_seq.shape}, test:{X_test_seq.shape}\")\n",
    "print(f\"Max seq length: {X_train_seq.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12499c4-42f8-428d-b5c7-09cf68401bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train non-pad length — mean:16.95, min:0, max:65\n",
      "Max token index in train: 39999\n"
     ]
    }
   ],
   "source": [
    "# Basic sequence stats to spot edge cases\n",
    "non_zero = np.count_nonzero(X_train_seq, axis=1)\n",
    "print(f\"Train non-pad length — mean:{non_zero.mean():.2f}, min:{non_zero.min()}, max:{non_zero.max()}\")\n",
    "\n",
    "# Token index upper bound (vocab cap used during sequencing)\n",
    "print(f\"Max token index in train: {int(X_train_seq.max())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf681d1-9f7c-4297-9af1-394b8888942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 12:17:26.040491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer size (unique indices): 46273\n",
      "Meta: VOCAB_SIZE=40000 | MAX_LEN=65\n"
     ]
    }
   ],
   "source": [
    "# Reuse the exact tokenizer used for training\n",
    "tokenizer = joblib.load(SEQ_PATH / \"tokenizer_v1.joblib\")\n",
    "\n",
    "# Metadata (e.g., VOCAB_SIZE, MAX_LEN)\n",
    "with open(SEQ_PATH / \"sequence_meta_v1.txt\", \"r\") as f:\n",
    "    meta_txt = f.read()\n",
    "\n",
    "print(f\"Tokenizer size (unique indices): {len(tokenizer.word_index)}\")\n",
    "print(\"Meta:\", meta_txt.strip().replace(\"\\n\", \" | \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350c295a-e5a2-4e65-b6d1-7a64771291c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shapes — train:(109815, 1), valid:(6101, 1), test:(6101, 1)\n",
      "Train class counts: {0: 14800, 1: 2174, 2: 3953, 3: 31351, 4: 36691, 5: 9130, 6: 11716}\n"
     ]
    }
   ],
   "source": [
    "# Load label CSVs (single column with integer class IDs)\n",
    "y_train = pd.read_csv(LABEL_PATH / \"y_train_v1.csv\")\n",
    "y_valid = pd.read_csv(LABEL_PATH / \"y_valid_v1.csv\")\n",
    "y_test  = pd.read_csv(LABEL_PATH / \"y_test_v1.csv\")\n",
    "\n",
    "label_col = y_train.columns[0]\n",
    "print(f\"Label shapes — train:{y_train.shape}, valid:{y_valid.shape}, test:{y_test.shape}\")\n",
    "\n",
    "# Light class distribution (train only)\n",
    "train_counts = y_train[label_col].value_counts().sort_index()\n",
    "print(\"Train class counts:\", train_counts.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b3a57c-ca79-4f6c-9cc9-9750a45e50d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'happiness', 4: 'neutral', 5: 'sadness', 6: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "# Mapping between emotion name and numeric ID\n",
    "with open(LABEL_PATH / \"label_mapping_v1.json\", \"r\") as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "# Make a reverse lookup for readability\n",
    "id_to_emotion = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "print(\"Classes:\", {i: id_to_emotion[i] for i in sorted(id_to_emotion)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccca1c41-7d75-4f72-85d8-68d7534f1555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense features: (122017, 22), n_cols=22\n"
     ]
    }
   ],
   "source": [
    "# Load engineered dense features and the scaler used at train time\n",
    "dense_features = pd.read_parquet(DENSE_PATH / \"dense_features_v1.parquet\", engine=\"pyarrow\")\n",
    "with open(DENSE_PATH / \"dense_feature_columns_v1.json\", \"r\") as f:\n",
    "    feature_columns = json.load(f)\n",
    "scaler = joblib.load(DENSE_PATH / \"dense_scaler_v1.joblib\")\n",
    "\n",
    "print(f\"Dense features: {dense_features.shape}, n_cols={len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a78ac5a-8efd-49d7-90f0-2357dfdf7b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense split — train:(109815, 22), valid:(6101, 22), test:(6101, 22)\n"
     ]
    }
   ],
   "source": [
    "# If there is no explicit 'split' column, assume concatenated [train|valid|test] order\n",
    "if \"split\" in dense_features.columns:\n",
    "    train_dense = dense_features[dense_features[\"split\"] == \"train\"][feature_columns]\n",
    "    valid_dense = dense_features[dense_features[\"split\"] == \"valid\"][feature_columns]\n",
    "    test_dense  = dense_features[dense_features[\"split\"] == \"test\"][feature_columns]\n",
    "else:\n",
    "    n_train, n_valid = len(y_train), len(y_valid)\n",
    "    train_dense = dense_features.iloc[:n_train][feature_columns]\n",
    "    valid_dense = dense_features.iloc[n_train:n_train+n_valid][feature_columns]\n",
    "    test_dense  = dense_features.iloc[n_train+n_valid:][feature_columns]\n",
    "\n",
    "print(f\"Dense split — train:{train_dense.shape}, valid:{valid_dense.shape}, test:{test_dense.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7149e9-5644-4769-8c00-ad94dd1fdd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq/label match: {'train': False, 'valid': False, 'test': False}\n",
      "Post-fix shapes — train:(109815, 65), valid:(6101, 65), test:(6101, 65)\n"
     ]
    }
   ],
   "source": [
    "# Check counts between sequences and labels\n",
    "def _ok(x, y): return x == y\n",
    "print(\"Seq/label match:\",\n",
    "      dict(train=_ok(X_train_seq.shape[0], len(y_train)),\n",
    "           valid=_ok(X_valid_seq.shape[0], len(y_valid)),\n",
    "           test=_ok(X_test_seq.shape[0], len(y_test))))\n",
    "\n",
    "# If sequences have one extra row per split, trim tail to match labels\n",
    "if X_train_seq.shape[0] != len(y_train):\n",
    "    X_train_seq = X_train_seq[:len(y_train)]\n",
    "if X_valid_seq.shape[0] != len(y_valid):\n",
    "    X_valid_seq = X_valid_seq[:len(y_valid)]\n",
    "if X_test_seq.shape[0]  != len(y_test):\n",
    "    X_test_seq  = X_test_seq[:len(y_test)]\n",
    "\n",
    "print(f\"Post-fix shapes — train:{X_train_seq.shape}, valid:{X_valid_seq.shape}, test:{X_test_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea02f52a-c964-4859-8ff4-0dab0ebfa256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train after removing empties: (109811, 65), labels:(109811, 1), dense:(109811, 22)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with 0 effective tokens (all pads) from TRAIN ONLY to avoid noise\n",
    "train_nonempty_mask = np.count_nonzero(X_train_seq, axis=1) > 0\n",
    "X_train_seq = X_train_seq[train_nonempty_mask]\n",
    "y_train = y_train.loc[train_nonempty_mask].reset_index(drop=True)\n",
    "train_dense = train_dense.loc[train_nonempty_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train after removing empties: {X_train_seq.shape}, labels:{y_train.shape}, dense:{train_dense.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f151a37f-cca6-4d28-86a7-76d2fb7fb67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External samples: 5994, text column: 'Translation'\n"
     ]
    }
   ],
   "source": [
    "# Load external test CSV with text and emotion labels\n",
    "new_test_data = pd.read_csv(\"group_8_url_1_transcript.csv\")\n",
    "\n",
    "# Use the same language as training; your tokenizer indicates English\n",
    "TEXT_COLUMN = \"Translation\"  # change to 'Sentence' if you actually trained on Arabic\n",
    "print(f\"External samples: {len(new_test_data)}, text column: '{TEXT_COLUMN}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e32750-51df-4ab6-9e67-7a4f7972cdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External samples: 5994, text column: 'Translation'\n"
     ]
    }
   ],
   "source": [
    "# Load external test CSV with text and emotion labels\n",
    "new_test_data = pd.read_csv(\"group_8_url_1_transcript.csv\")\n",
    "\n",
    "# Use the same language as training; your tokenizer indicates English\n",
    "TEXT_COLUMN = \"Translation\"  # change to 'Sentence' if you actually trained on Arabic\n",
    "print(f\"External samples: {len(new_test_data)}, text column: '{TEXT_COLUMN}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eff0d25-58fb-47d7-80cf-984919846cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from each column:\n",
      "\n",
      "\n",
      "Start Time:\n",
      "0    1900-01-01 00:00:00\n",
      "1    1900-01-01 00:00:02\n",
      "2    1900-01-01 00:00:04\n",
      "Name: Start Time, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "End Time:\n",
      "0    1900-01-01 00:00:02\n",
      "1    1900-01-01 00:00:04\n",
      "2    1900-01-01 00:00:05\n",
      "Name: End Time, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Sentence:\n",
      "0    لا يوجد علاقة بدون حاجة\n",
      "1                      قاعدة\n",
      "2              لا يوجد علاقة\n",
      "Name: Sentence, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Translation:\n",
      "0    There is no relationship without need\n",
      "1                                     Base\n",
      "2                 There is no relationship\n",
      "Name: Translation, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Emotion_fine:\n",
      "0    resignation\n",
      "1     neutrality\n",
      "2     detachment\n",
      "Name: Emotion_fine, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Emotion_core:\n",
      "0    neutral\n",
      "1    neutral\n",
      "2    neutral\n",
      "Name: Emotion_core, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Intensity:\n",
      "0    neutral\n",
      "1    neutral\n",
      "2    neutral\n",
      "Name: Intensity, dtype: object\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show sample of each column to identify which contains text\n",
    "print(\"Sample data from each column:\\n\")\n",
    "for col in new_test_data.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(new_test_data[col].head(3))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dac185d-df38-47c3-86b4-9cbf5595b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External tokenized shape: (5994, 65) (seq_len=65)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Convert raw text to sequences using the training tokenizer\n",
    "test_texts = new_test_data[TEXT_COLUMN].fillna(\"\").astype(str).tolist()\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Pad/truncate to the training max length\n",
    "SEQ_LEN = X_train_seq.shape[1]\n",
    "X_new_test = pad_sequences(test_sequences, maxlen=SEQ_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(f\"External tokenized shape: {X_new_test.shape} (seq_len={SEQ_LEN})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48de5f10-c83c-41c9-90cf-2cee5416f068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External labels: total=5994, valid=5994, unknown=0\n",
      "Emotion column used: Emotion_core\n"
     ]
    }
   ],
   "source": [
    "# The external file includes fine-grained and core emotions; map the core ones to your 7-class set\n",
    "if set(new_test_data.get(\"Emotion_core\", [])).issubset(set(label_mapping.keys())):\n",
    "    EMOTION_COL = \"Emotion_core\"\n",
    "elif set(new_test_data.get(\"Emotion_fine\", [])).issubset(set(label_mapping.keys())):\n",
    "    EMOTION_COL = \"Emotion_fine\"\n",
    "else:\n",
    "    # Default to core; if there are unknowns, they’ll be flagged below\n",
    "    EMOTION_COL = \"Emotion_core\"\n",
    "\n",
    "# Convert emotion strings to numeric IDs (unknowns -> -1)\n",
    "y_new_test = []\n",
    "for emo in new_test_data[EMOTION_COL].astype(str):\n",
    "    y_new_test.append(label_mapping.get(emo, -1))\n",
    "y_new_test = np.array(y_new_test, dtype=int)\n",
    "\n",
    "valid_mask = y_new_test >= 0\n",
    "X_new_test_valid = X_new_test[valid_mask]\n",
    "y_new_test_valid = y_new_test[valid_mask]\n",
    "\n",
    "print(f\"External labels: total={len(y_new_test)}, valid={valid_mask.sum()}, unknown={(~valid_mask).sum()}\")\n",
    "print(f\"Emotion column used: {EMOTION_COL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65bcf048-7fc0-4373-a122-784abe1b22ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_seq': (109811, 65),\n",
       " 'valid_seq': (6101, 65),\n",
       " 'test_seq': (6101, 65),\n",
       " 'dense_cols': 22,\n",
       " 'new_test_seq': (5994, 65),\n",
       " 'new_test_valid': 5994,\n",
       " 'n_classes': 7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compact one-liner summaries to keep the notebook quiet but informative\n",
    "summary = {\n",
    "    \"train_seq\": X_train_seq.shape,\n",
    "    \"valid_seq\": X_valid_seq.shape,\n",
    "    \"test_seq\":  X_test_seq.shape,\n",
    "    \"dense_cols\": len(feature_columns),\n",
    "    \"new_test_seq\": X_new_test.shape,\n",
    "    \"new_test_valid\": X_new_test_valid.shape[0],\n",
    "    \"n_classes\": len(label_mapping),\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a01a7393-c1a7-46ea-b93a-07aeb7a5b4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: test_data_prepared.csv, X_new_test.npz, y_new_test.npy\n"
     ]
    }
   ],
   "source": [
    "# Save external test set artifacts for later prediction/evaluation\n",
    "new_test_data.assign(\n",
    "    is_valid=valid_mask,\n",
    "    numeric_label=y_new_test\n",
    ").to_csv(\"test_data_prepared.csv\", index=False)\n",
    "\n",
    "np.savez_compressed(\"X_new_test.npz\", X_new_test_valid)\n",
    "np.save(\"y_new_test.npy\", y_new_test_valid)\n",
    "\n",
    "print(\"Saved: test_data_prepared.csv, X_new_test.npz, y_new_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a3a393c-9fc3-4a44-add5-d4f41a97c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for prediction/evaluation. (See commented example.)\n"
     ]
    }
   ],
   "source": [
    "# Example usage after you load your trained model:\n",
    "# predictions = model.predict({\"tokens\": X_new_test_valid, \"dense\": scaled_dense_if_used})\n",
    "# from sklearn.metrics import classification_report\n",
    "# pred_labels = predictions.argmax(axis=1)\n",
    "# print(classification_report(y_new_test_valid, pred_labels, target_names=[id_to_emotion[i] for i in range(len(label_mapping))]))\n",
    "print(\"Ready for prediction/evaluation. (See commented example.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863a952-9835-4269-b407-c8e869ce2595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
