{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix pyarrow/pandas compatibility issue\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready.\n"
     ]
    }
   ],
   "source": [
    "# Core imports and paths\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust these paths as needed\n",
    "SEQ_PATH   = Path(\"artifacts/features/sequences/\")\n",
    "LABEL_PATH = Path(\"artifacts/labels/\")\n",
    "DENSE_PATH = Path(\"artifacts/features/dense/\")\n",
    "\n",
    "print(\"Environment ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq shapes — train:(109816, 65), valid:(6102, 65), test:(6102, 65)\n",
      "Max seq length: 65\n"
     ]
    }
   ],
   "source": [
    "# Load tokenized sequences (npz with 'arr_0')\n",
    "X_train_seq = np.load(SEQ_PATH / \"X_train_seq_v1.npz\")[\"arr_0\"]\n",
    "X_valid_seq = np.load(SEQ_PATH / \"X_valid_seq_v1.npz\")[\"arr_0\"]\n",
    "X_test_seq  = np.load(SEQ_PATH / \"X_test_seq_v1.npz\")[\"arr_0\"]\n",
    "\n",
    "# Minimal sanity info\n",
    "print(f\"Seq shapes — train:{X_train_seq.shape}, valid:{X_valid_seq.shape}, test:{X_test_seq.shape}\")\n",
    "print(f\"Max seq length: {X_train_seq.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train non-pad length — mean:16.95, min:0, max:65\n",
      "Max token index in train: 39999\n"
     ]
    }
   ],
   "source": [
    "# Basic sequence stats to spot edge cases\n",
    "non_zero = np.count_nonzero(X_train_seq, axis=1)\n",
    "print(f\"Train non-pad length — mean:{non_zero.mean():.2f}, min:{non_zero.min()}, max:{non_zero.max()}\")\n",
    "\n",
    "# Token index upper bound (vocab cap used during sequencing)\n",
    "print(f\"Max token index in train: {int(X_train_seq.max())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 19:47:41.209876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer size (unique indices): 46273\n",
      "Meta: VOCAB_SIZE=40000 | MAX_LEN=65\n"
     ]
    }
   ],
   "source": [
    "# Reuse the exact tokenizer used for training\n",
    "tokenizer = joblib.load(SEQ_PATH / \"tokenizer_v1.joblib\")\n",
    "\n",
    "# Metadata (e.g., VOCAB_SIZE, MAX_LEN)\n",
    "with open(SEQ_PATH / \"sequence_meta_v1.txt\", \"r\") as f:\n",
    "    meta_txt = f.read()\n",
    "\n",
    "print(f\"Tokenizer size (unique indices): {len(tokenizer.word_index)}\")\n",
    "print(\"Meta:\", meta_txt.strip().replace(\"\\n\", \" | \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shapes — train:(109815, 1), valid:(6101, 1), test:(6101, 1)\n",
      "Train class counts: {0: 14800, 1: 2174, 2: 3953, 3: 31351, 4: 36691, 5: 9130, 6: 11716}\n"
     ]
    }
   ],
   "source": [
    "# Load label CSVs (single column with integer class IDs)\n",
    "y_train = pd.read_csv(LABEL_PATH / \"y_train_v1.csv\")\n",
    "y_valid = pd.read_csv(LABEL_PATH / \"y_valid_v1.csv\")\n",
    "y_test  = pd.read_csv(LABEL_PATH / \"y_test_v1.csv\")\n",
    "\n",
    "label_col = y_train.columns[0]\n",
    "print(f\"Label shapes — train:{y_train.shape}, valid:{y_valid.shape}, test:{y_test.shape}\")\n",
    "\n",
    "# Light class distribution (train only)\n",
    "train_counts = y_train[label_col].value_counts().sort_index()\n",
    "print(\"Train class counts:\", train_counts.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'happiness', 4: 'neutral', 5: 'sadness', 6: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "# Mapping between emotion name and numeric ID\n",
    "with open(LABEL_PATH / \"label_mapping_v1.json\", \"r\") as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "# Make a reverse lookup for readability\n",
    "id_to_emotion = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "print(\"Classes:\", {i: id_to_emotion[i] for i in sorted(id_to_emotion)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense features: (122017, 22), n_cols=22\n"
     ]
    }
   ],
   "source": [
    "# Load engineered dense features and the scaler used at train time\n",
    "dense_features = pd.read_parquet(DENSE_PATH / \"dense_features_v1.parquet\", engine=\"pyarrow\")\n",
    "with open(DENSE_PATH / \"dense_feature_columns_v1.json\", \"r\") as f:\n",
    "    feature_columns = json.load(f)\n",
    "scaler = joblib.load(DENSE_PATH / \"dense_scaler_v1.joblib\")\n",
    "\n",
    "print(f\"Dense features: {dense_features.shape}, n_cols={len(feature_columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense split — train:(109815, 22), valid:(6101, 22), test:(6101, 22)\n"
     ]
    }
   ],
   "source": [
    "# If there is no explicit 'split' column, assume concatenated [train|valid|test] order\n",
    "if \"split\" in dense_features.columns:\n",
    "    train_dense = dense_features[dense_features[\"split\"] == \"train\"][feature_columns]\n",
    "    valid_dense = dense_features[dense_features[\"split\"] == \"valid\"][feature_columns]\n",
    "    test_dense  = dense_features[dense_features[\"split\"] == \"test\"][feature_columns]\n",
    "else:\n",
    "    n_train, n_valid = len(y_train), len(y_valid)\n",
    "    train_dense = dense_features.iloc[:n_train][feature_columns]\n",
    "    valid_dense = dense_features.iloc[n_train:n_train+n_valid][feature_columns]\n",
    "    test_dense  = dense_features.iloc[n_train+n_valid:][feature_columns]\n",
    "\n",
    "print(f\"Dense split — train:{train_dense.shape}, valid:{valid_dense.shape}, test:{test_dense.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq/label match: {'train': False, 'valid': False, 'test': False}\n",
      "Post-fix shapes — train:(109815, 65), valid:(6101, 65), test:(6101, 65)\n"
     ]
    }
   ],
   "source": [
    "# Check counts between sequences and labels\n",
    "def _ok(x, y): return x == y\n",
    "print(\"Seq/label match:\",\n",
    "      dict(train=_ok(X_train_seq.shape[0], len(y_train)),\n",
    "           valid=_ok(X_valid_seq.shape[0], len(y_valid)),\n",
    "           test=_ok(X_test_seq.shape[0], len(y_test))))\n",
    "\n",
    "# If sequences have one extra row per split, trim tail to match labels\n",
    "if X_train_seq.shape[0] != len(y_train):\n",
    "    X_train_seq = X_train_seq[:len(y_train)]\n",
    "if X_valid_seq.shape[0] != len(y_valid):\n",
    "    X_valid_seq = X_valid_seq[:len(y_valid)]\n",
    "if X_test_seq.shape[0]  != len(y_test):\n",
    "    X_test_seq  = X_test_seq[:len(y_test)]\n",
    "\n",
    "print(f\"Post-fix shapes — train:{X_train_seq.shape}, valid:{X_valid_seq.shape}, test:{X_test_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train after removing empties: (109811, 65), labels:(109811, 1), dense:(109811, 22)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with 0 effective tokens (all pads) from TRAIN ONLY to avoid noise\n",
    "train_nonempty_mask = np.count_nonzero(X_train_seq, axis=1) > 0\n",
    "X_train_seq = X_train_seq[train_nonempty_mask]\n",
    "y_train = y_train.loc[train_nonempty_mask].reset_index(drop=True)\n",
    "train_dense = train_dense.loc[train_nonempty_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train after removing empties: {X_train_seq.shape}, labels:{y_train.shape}, dense:{train_dense.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External samples: 5994, text column: 'Translation'\n"
     ]
    }
   ],
   "source": [
    "# Load external test CSV with text and emotion labels\n",
    "new_test_data = pd.read_csv(\"group_8_url_1_transcript.csv\")\n",
    "\n",
    "# Use the same language as training; your tokenizer indicates English\n",
    "TEXT_COLUMN = \"Translation\"  # change to 'Sentence' if you actually trained on Arabic\n",
    "print(f\"External samples: {len(new_test_data)}, text column: '{TEXT_COLUMN}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External samples: 5994, text column: 'Translation'\n"
     ]
    }
   ],
   "source": [
    "# Load external test CSV with text and emotion labels\n",
    "new_test_data = pd.read_csv(\"group_8_url_1_transcript.csv\")\n",
    "\n",
    "# Use the same language as training; your tokenizer indicates English\n",
    "TEXT_COLUMN = \"Translation\"  # change to 'Sentence' if you actually trained on Arabic\n",
    "print(f\"External samples: {len(new_test_data)}, text column: '{TEXT_COLUMN}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from each column:\n",
      "\n",
      "\n",
      "Start Time:\n",
      "0    1900-01-01 00:00:00\n",
      "1    1900-01-01 00:00:02\n",
      "2    1900-01-01 00:00:04\n",
      "Name: Start Time, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "End Time:\n",
      "0    1900-01-01 00:00:02\n",
      "1    1900-01-01 00:00:04\n",
      "2    1900-01-01 00:00:05\n",
      "Name: End Time, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Sentence:\n",
      "0    لا يوجد علاقة بدون حاجة\n",
      "1                      قاعدة\n",
      "2              لا يوجد علاقة\n",
      "Name: Sentence, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Translation:\n",
      "0    There is no relationship without need\n",
      "1                                     Base\n",
      "2                 There is no relationship\n",
      "Name: Translation, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Emotion_fine:\n",
      "0    resignation\n",
      "1     neutrality\n",
      "2     detachment\n",
      "Name: Emotion_fine, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Emotion_core:\n",
      "0    neutral\n",
      "1    neutral\n",
      "2    neutral\n",
      "Name: Emotion_core, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Intensity:\n",
      "0    neutral\n",
      "1    neutral\n",
      "2    neutral\n",
      "Name: Intensity, dtype: object\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show sample of each column to identify which contains text\n",
    "print(\"Sample data from each column:\\n\")\n",
    "for col in new_test_data.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(new_test_data[col].head(3))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External tokenized shape: (5994, 65) (seq_len=65)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Convert raw text to sequences using the training tokenizer\n",
    "test_texts = new_test_data[TEXT_COLUMN].fillna(\"\").astype(str).tolist()\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Pad/truncate to the training max length\n",
    "SEQ_LEN = X_train_seq.shape[1]\n",
    "X_new_test = pad_sequences(test_sequences, maxlen=SEQ_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(f\"External tokenized shape: {X_new_test.shape} (seq_len={SEQ_LEN})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External labels: total=5994, valid=5994, unknown=0\n",
      "Emotion column used: Emotion_core\n"
     ]
    }
   ],
   "source": [
    "# The external file includes fine-grained and core emotions; map the core ones to your 7-class set\n",
    "if set(new_test_data.get(\"Emotion_core\", [])).issubset(set(label_mapping.keys())):\n",
    "    EMOTION_COL = \"Emotion_core\"\n",
    "elif set(new_test_data.get(\"Emotion_fine\", [])).issubset(set(label_mapping.keys())):\n",
    "    EMOTION_COL = \"Emotion_fine\"\n",
    "else:\n",
    "    # Default to core; if there are unknowns, they’ll be flagged below\n",
    "    EMOTION_COL = \"Emotion_core\"\n",
    "\n",
    "# Convert emotion strings to numeric IDs (unknowns -> -1)\n",
    "y_new_test = []\n",
    "for emo in new_test_data[EMOTION_COL].astype(str):\n",
    "    y_new_test.append(label_mapping.get(emo, -1))\n",
    "y_new_test = np.array(y_new_test, dtype=int)\n",
    "\n",
    "valid_mask = y_new_test >= 0\n",
    "X_new_test_valid = X_new_test[valid_mask]\n",
    "y_new_test_valid = y_new_test[valid_mask]\n",
    "\n",
    "print(f\"External labels: total={len(y_new_test)}, valid={valid_mask.sum()}, unknown={(~valid_mask).sum()}\")\n",
    "print(f\"Emotion column used: {EMOTION_COL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_seq': (109811, 65),\n",
       " 'valid_seq': (6101, 65),\n",
       " 'test_seq': (6101, 65),\n",
       " 'dense_cols': 22,\n",
       " 'new_test_seq': (5994, 65),\n",
       " 'new_test_valid': 5994,\n",
       " 'n_classes': 7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compact one-liner summaries to keep the notebook quiet but informative\n",
    "summary = {\n",
    "    \"train_seq\": X_train_seq.shape,\n",
    "    \"valid_seq\": X_valid_seq.shape,\n",
    "    \"test_seq\":  X_test_seq.shape,\n",
    "    \"dense_cols\": len(feature_columns),\n",
    "    \"new_test_seq\": X_new_test.shape,\n",
    "    \"new_test_valid\": X_new_test_valid.shape[0],\n",
    "    \"n_classes\": len(label_mapping),\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: test_data_prepared.csv, X_new_test.npz, y_new_test.npy\n"
     ]
    }
   ],
   "source": [
    "# Save external test set artifacts for later prediction/evaluation\n",
    "new_test_data.assign(\n",
    "    is_valid=valid_mask,\n",
    "    numeric_label=y_new_test\n",
    ").to_csv(\"test_data_prepared.csv\", index=False)\n",
    "\n",
    "np.savez_compressed(\"X_new_test.npz\", X_new_test_valid)\n",
    "np.save(\"y_new_test.npy\", y_new_test_valid)\n",
    "\n",
    "print(\"Saved: test_data_prepared.csv, X_new_test.npz, y_new_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for prediction/evaluation. (See commented example.)\n"
     ]
    }
   ],
   "source": [
    "# Example usage after you load your trained model:\n",
    "# predictions = model.predict({\"tokens\": X_new_test_valid, \"dense\": scaled_dense_if_used})\n",
    "# from sklearn.metrics import classification_report\n",
    "# pred_labels = predictions.argmax(axis=1)\n",
    "# print(classification_report(y_new_test_valid, pred_labels, target_names=[id_to_emotion[i] for i in range(len(label_mapping))]))\n",
    "print(\"Ready for prediction/evaluation. (See commented example.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqLen=65, Classes=7, Vocab=40000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L, models as M\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Targets (assumes y_train/y_valid/y_test from your prep)\n",
    "label_col = y_train.columns[0]\n",
    "y_tr = y_train[label_col].to_numpy(dtype=np.int32).ravel()\n",
    "y_va = y_valid[label_col].to_numpy(dtype=np.int32).ravel()\n",
    "y_te = y_test[label_col].to_numpy(dtype=np.int32).ravel()\n",
    "\n",
    "# Dimensions\n",
    "SEQ_LEN = int(X_train_seq.shape[1])\n",
    "N_CLASSES = int(len(set(y_tr)))\n",
    "VOCAB_SIZE = int(X_train_seq.max()) + 1  # includes pad=0\n",
    "\n",
    "print(f\"SeqLen={SEQ_LEN}, Classes={N_CLASSES}, Vocab={VOCAB_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 19:51:20.704167: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759521080.705198   36645 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16184 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:25:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                   </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">          Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ tokens (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                │                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │ tokens[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ spatial_dropout1d              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)             │                           │                  │                            │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                │                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ tokens[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">172,800</span> │ spatial_dropout1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                                │                           │                  │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)               │                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)           │                           │                  │                            │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)               │                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)       │                           │                  │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)               │                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                                │                           │                  │ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└────────────────────────────────┴───────────────────────────┴──────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m         Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ tokens (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                │                \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │        \u001b[38;5;34m5,120,000\u001b[0m │ tokens[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ spatial_dropout1d              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │                \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)             │                           │                  │                            │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                │                \u001b[38;5;34m0\u001b[0m │ tokens[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m192\u001b[0m)           │          \u001b[38;5;34m172,800\u001b[0m │ spatial_dropout1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                                │                           │                  │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)               │                \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)           │                           │                  │                            │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)               │                \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)       │                           │                  │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)               │                \u001b[38;5;34m0\u001b[0m │ global_max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                                │                           │                  │ global_average_pooling1d[\u001b[38;5;34m…\u001b[0m │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │           \u001b[38;5;34m49,280\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │                \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├────────────────────────────────┼───────────────────────────┼──────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                 │              \u001b[38;5;34m903\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└────────────────────────────────┴───────────────────────────┴──────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,342,983</span> (20.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,342,983\u001b[0m (20.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,342,983</span> (20.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,342,983\u001b[0m (20.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model(vocab_size, seq_len, n_classes, emb_dim=128, lstm_units=96, dropout=0.4):\n",
    "    inp = L.Input(shape=(seq_len,), name=\"tokens\")\n",
    "    x = L.Embedding(vocab_size, emb_dim, mask_zero=True)(inp)\n",
    "    x = L.SpatialDropout1D(0.2)(x)\n",
    "    x = L.Bidirectional(L.LSTM(lstm_units, return_sequences=True))(x)\n",
    "    x = L.Concatenate()([L.GlobalMaxPooling1D()(x), L.GlobalAveragePooling1D()(x)])\n",
    "    x = L.Dense(128, activation=\"relu\")(x)\n",
    "    x = L.Dropout(dropout)(x)\n",
    "    out = L.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    model = M.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, SEQ_LEN, N_CLASSES)\n",
    "model.summary(line_length=110, expand_nested=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 19:51:43.067469: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m214/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.1213 - loss: 1.9455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - acc: 0.1144 - loss: 1.9463 - val_acc: 0.1700 - val_loss: 1.9442 - learning_rate: 3.0000e-04\n",
      "Epoch 2/12\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.1624 - loss: 1.9437"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - acc: 0.1258 - loss: 1.9436 - val_acc: 0.1321 - val_loss: 1.9227 - learning_rate: 3.0000e-04\n",
      "Epoch 3/12\n",
      "\u001b[1m213/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.1370 - loss: 1.9206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - acc: 0.1323 - loss: 1.9040 - val_acc: 0.1329 - val_loss: 1.8885 - learning_rate: 3.0000e-04\n",
      "Epoch 4/12\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - acc: 0.1786 - loss: 1.7623 - val_acc: 0.1288 - val_loss: 1.9185 - learning_rate: 3.0000e-04\n",
      "Epoch 5/12\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - acc: 0.2221 - loss: 1.6136 - val_acc: 0.1374 - val_loss: 1.9481 - learning_rate: 3.0000e-04\n",
      "Epoch 6/12\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - acc: 0.2680 - loss: 1.4713 - val_acc: 0.1474 - val_loss: 2.0142 - learning_rate: 1.5000e-04\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: 'NotEqual'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Reload best by val_loss (optional but safe)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[33m\"\u001b[39m\u001b[33mbaseline_bilstm_best.h5\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     model = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbaseline_bilstm_best.h5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     model.compile(optimizer=tf.keras.optimizers.Adam(\u001b[32m3e-4\u001b[39m),\n\u001b[32m     28\u001b[39m                   loss=\u001b[33m\"\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m                   metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\u001b[33m\"\u001b[39m\u001b[33macc\u001b[39m\u001b[33m\"\u001b[39m)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py:196\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib.load_model(\n\u001b[32m    190\u001b[39m         filepath,\n\u001b[32m    191\u001b[39m         custom_objects=custom_objects,\n\u001b[32m    192\u001b[39m         \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28mcompile\u001b[39m,\n\u001b[32m    193\u001b[39m         safe_mode=safe_mode,\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith((\u001b[33m\"\u001b[39m\u001b[33m.h5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.hdf5\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py:132\u001b[39m, in \u001b[36mload_model_from_hdf5\u001b[39m\u001b[34m(filepath, custom_objects, compile)\u001b[39m\n\u001b[32m    129\u001b[39m model_config = json_utils.decode(model_config)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m saving_options.keras_option_scope(use_legacy_config=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     model = \u001b[43msaving_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[32m    137\u001b[39m     load_weights_from_hdf5_group(f[\u001b[33m\"\u001b[39m\u001b[33mmodel_weights\u001b[39m\u001b[33m\"\u001b[39m], model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/saving_utils.py:88\u001b[39m, in \u001b[36mmodel_from_config\u001b[39m\u001b[34m(config, custom_objects)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[32m     86\u001b[39m config = _find_replace_nested_dict(config, \u001b[33m\"\u001b[39m\u001b[33mkeras.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkeras.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlayer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/serialization.py:495\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[39m\n\u001b[32m    490\u001b[39m cls_config = _find_replace_nested_dict(\n\u001b[32m    491\u001b[39m     cls_config, \u001b[33m\"\u001b[39m\u001b[33mkeras.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkeras.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    492\u001b[39m )\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcustom_objects\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec.args:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     deserialized_obj = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mobject_registration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration.CustomObjectScope(custom_objects):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py:660\u001b[39m, in \u001b[36mModel.from_config\u001b[39m\u001b[34m(cls, config, custom_objects)\u001b[39m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[32m    657\u001b[39m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:558\u001b[39m, in \u001b[36mfunctional_from_config\u001b[39m\u001b[34m(cls, config, custom_objects)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m functional_config[\u001b[33m\"\u001b[39m\u001b[33mlayers\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:521\u001b[39m, in \u001b[36mfunctional_from_config.<locals>.process_layer\u001b[39m\u001b[34m(layer_data)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# Instantiate layer.\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_data:\n\u001b[32m    519\u001b[39m     \u001b[38;5;66;03m# Legacy format deserialization (no \"module\" key)\u001b[39;00m\n\u001b[32m    520\u001b[39m     \u001b[38;5;66;03m# used for H5 and SavedModel formats\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     layer = \u001b[43msaving_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    525\u001b[39m     layer = serialization_lib.deserialize_keras_object(\n\u001b[32m    526\u001b[39m         layer_data, custom_objects=custom_objects\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/saving_utils.py:88\u001b[39m, in \u001b[36mmodel_from_config\u001b[39m\u001b[34m(config, custom_objects)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[32m     86\u001b[39m config = _find_replace_nested_dict(config, \u001b[33m\"\u001b[39m\u001b[33mkeras.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkeras.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlayer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/serialization.py:473\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# In this case we are dealing with a Keras config dictionary.\u001b[39;00m\n\u001b[32m    472\u001b[39m     config = identifier\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     (\u001b[38;5;28mcls\u001b[39m, cls_config) = \u001b[43mclass_and_config_for_serialized_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintable_module_name\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# If this object has already been loaded (i.e. it's shared between\u001b[39;00m\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# multiple objects), return the already-loaded object.\u001b[39;00m\n\u001b[32m    479\u001b[39m     shared_object_id = config.get(SHARED_OBJECT_KEY)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/serialization.py:354\u001b[39m, in \u001b[36mclass_and_config_for_serialized_keras_object\u001b[39m\u001b[34m(config, module_objects, custom_objects, printable_module_name)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;28mcls\u001b[39m = object_registration.get_registered_object(\n\u001b[32m    351\u001b[39m     class_name, custom_objects, module_objects\n\u001b[32m    352\u001b[39m )\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    355\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    356\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    357\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand that this object is included in the scope. See \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    358\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#registering_the_custom_object for details.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m     )\n\u001b[32m    362\u001b[39m cls_config = config[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;66;03m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Unknown layer: 'NotEqual'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "# Class weights for imbalance\n",
    "classes = np.arange(N_CLASSES)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "class_weights = {i: float(w) for i, w in enumerate(cw)}\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=0),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"baseline_bilstm_best.h5\", monitor=\"val_loss\",\n",
    "                                       save_best_only=True, save_weights_only=False, verbose=0),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2,\n",
    "                                         min_lr=1e-6, verbose=0),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_tr,\n",
    "    validation_data=(X_valid_seq, y_va),\n",
    "    epochs=12,\n",
    "    batch_size=512,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reload best by val_loss (optional but safe)\n",
    "if os.path.exists(\"baseline_bilstm_best.h5\"):\n",
    "    model = tf.keras.models.load_model(\"baseline_bilstm_best.h5\", compile=False)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal test\n",
    "pred_te = model.predict(X_test_seq, verbose=0).argmax(axis=1)\n",
    "print(\"Internal test:\")\n",
    "print(classification_report(y_te, pred_te, digits=3, zero_division=0))\n",
    "\n",
    "# External test (from your saved prep files)\n",
    "X_new = np.load(\"X_new_test.npz\")[\"arr_0\"]   # saved with np.savez_compressed\n",
    "y_new = np.load(\"y_new_test.npy\")\n",
    "\n",
    "assert X_new.shape[1] == SEQ_LEN, \"External test seq length mismatch.\"\n",
    "\n",
    "pred_new = model.predict(X_new, verbose=0).argmax(axis=1)\n",
    "print(\"External test:\")\n",
    "print(classification_report(y_new, pred_new, digits=3, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
