{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix pyarrow/pandas compatibility issue\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Data path: artifacts/features/sequences\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your data path - CHANGE THIS to where your files are located\n",
    "DATA_PATH = Path('artifacts/features/sequences/')  # or wherever your files are\n",
    "# Example alternatives:\n",
    "# DATA_PATH = Path('/content/artifacts/features/sequences/')  # Colab\n",
    "# DATA_PATH = Path('C:/Users/YourName/data/')  # Windows\n",
    "# DATA_PATH = Path('./data/')  # Current directory\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Data path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Data Shapes:\n",
      "Train: (109816, 65)\n",
      "Valid: (6102, 65)\n",
      "Test:  (6102, 65)\n",
      "\n",
      "Sequence length: 65\n",
      "Total samples: 122020\n"
     ]
    }
   ],
   "source": [
    "# Load sequences\n",
    "X_train_seq = np.load(DATA_PATH / 'X_train_seq_v1.npz')['arr_0']\n",
    "X_valid_seq = np.load(DATA_PATH / 'X_valid_seq_v1.npz')['arr_0']\n",
    "X_test_seq = np.load(DATA_PATH / 'X_test_seq_v1.npz')['arr_0']\n",
    "\n",
    "print(\"Sequence Data Shapes:\")\n",
    "print(f\"Train: {X_train_seq.shape}\")\n",
    "print(f\"Valid: {X_valid_seq.shape}\")\n",
    "print(f\"Test:  {X_test_seq.shape}\")\n",
    "print(f\"\\nSequence length: {X_train_seq.shape[1]}\")\n",
    "print(f\"Total samples: {X_train_seq.shape[0] + X_valid_seq.shape[0] + X_test_seq.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sequence (first 20 tokens):\n",
      "[ 903   14  388    9   31  135 5091 7266  199 6163   42   49    0    0\n",
      "    0    0    0    0    0    0]\n",
      "\n",
      "Vocabulary size: 39999\n",
      "\n",
      "Average sequence length (non-padded): 16.95\n",
      "Min length: 0\n",
      "Max length: 65\n"
     ]
    }
   ],
   "source": [
    "# Sample sequence\n",
    "print(\"Sample sequence (first 20 tokens):\")\n",
    "print(X_train_seq[0][:20])\n",
    "\n",
    "# Vocabulary size\n",
    "print(f\"\\nVocabulary size: {X_train_seq.max()}\")\n",
    "\n",
    "# Average actual length (non-padded)\n",
    "non_zero = np.count_nonzero(X_train_seq, axis=1)\n",
    "print(f\"\\nAverage sequence length (non-padded): {non_zero.mean():.2f}\")\n",
    "print(f\"Min length: {non_zero.min()}\")\n",
    "print(f\"Max length: {non_zero.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 19:28:59.945303: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer type: <class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n",
      "Vocabulary size: 46273\n",
      "\n",
      "Sample words:\n",
      "  '<UNK>': 1\n",
      "  'the': 2\n",
      "  'i': 3\n",
      "  'you': 4\n",
      "  'to': 5\n",
      "  'a': 6\n",
      "  'and': 7\n",
      "  'it': 8\n",
      "  'is': 9\n",
      "  'that': 10\n",
      "  'of': 11\n",
      "  'in': 12\n",
      "  'for': 13\n",
      "  'this': 14\n",
      "  'name': 15\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = joblib.load(DATA_PATH / 'tokenizer_v1.joblib')\n",
    "\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "\n",
    "# Sample words from vocabulary\n",
    "print(\"\\nSample words:\")\n",
    "for word, idx in list(tokenizer.word_index.items())[:15]:\n",
    "    print(f\"  '{word}': {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Metadata:\n",
      "VOCAB_SIZE=40000\n",
      "MAX_LEN=65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "with open(DATA_PATH / 'sequence_meta_v1.txt', 'r') as f:\n",
    "    meta = f.read()\n",
    "    \n",
    "print(\"Sequence Metadata:\")\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Shapes:\n",
      "Train: (109815, 1)\n",
      "Valid: (6101, 1)\n",
      "Test:  (6101, 1)\n",
      "\n",
      "First few labels:\n",
      "   0\n",
      "0  3\n",
      "1  3\n",
      "2  4\n",
      "3  0\n",
      "4  0\n",
      "5  4\n",
      "6  3\n",
      "7  6\n",
      "8  6\n",
      "9  4\n",
      "\n",
      "Class distribution:\n",
      "0\n",
      "0    14800\n",
      "1     2174\n",
      "2     3953\n",
      "3    31351\n",
      "4    36691\n",
      "5     9130\n",
      "6    11716\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "0\n",
      "0    13.477212\n",
      "1     1.979693\n",
      "2     3.599690\n",
      "3    28.548923\n",
      "4    33.411647\n",
      "5     8.313983\n",
      "6    10.668852\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load labels - adjust path to where your label files are\n",
    "LABEL_PATH = Path('artifacts/labels/')  # CHANGE THIS\n",
    "\n",
    "y_train = pd.read_csv(LABEL_PATH / 'y_train_v1.csv')\n",
    "y_valid = pd.read_csv(LABEL_PATH / 'y_valid_v1.csv')\n",
    "y_test = pd.read_csv(LABEL_PATH / 'y_test_v1.csv')\n",
    "\n",
    "print(\"Label Shapes:\")\n",
    "print(f\"Train: {y_train.shape}\")\n",
    "print(f\"Valid: {y_valid.shape}\")\n",
    "print(f\"Test:  {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nFirst few labels:\")\n",
    "print(y_train.head(10))\n",
    "\n",
    "# Class distribution\n",
    "label_col = y_train.columns[0]\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y_train[label_col].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nPercentage distribution:\")\n",
    "print((y_train[label_col].value_counts(normalize=True) * 100).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping Structure:\n",
      "{'anger': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'neutral': 4, 'sadness': 5, 'surprise': 6}\n",
      "\n",
      "Emotion Labels (index: emotion):\n",
      "  0: anger\n",
      "  1: disgust\n",
      "  2: fear\n",
      "  3: happiness\n",
      "  4: neutral\n",
      "  5: sadness\n",
      "  6: surprise\n",
      "\n",
      "Emotion to Index Mapping:\n",
      "  'anger': 0\n",
      "  'disgust': 1\n",
      "  'fear': 2\n",
      "  'happiness': 3\n",
      "  'neutral': 4\n",
      "  'sadness': 5\n",
      "  'surprise': 6\n",
      "\n",
      "Number of emotion classes: 7\n"
     ]
    }
   ],
   "source": [
    "# Load label mapping\n",
    "with open(LABEL_PATH / 'label_mapping_v1.json', 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "print(\"Label Mapping Structure:\")\n",
    "print(label_mapping)\n",
    "\n",
    "# Check if it's emotion->index or index->emotion\n",
    "if isinstance(list(label_mapping.keys())[0], str) and list(label_mapping.keys())[0].isalpha():\n",
    "    # It's emotion->index format, reverse it\n",
    "    label_mapping_reversed = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    print(\"\\nEmotion Labels (index: emotion):\")\n",
    "    for idx, emotion in sorted(label_mapping_reversed.items()):\n",
    "        print(f\"  {idx}: {emotion}\")\n",
    "    \n",
    "    print(\"\\nEmotion to Index Mapping:\")\n",
    "    for emotion, idx in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
    "        print(f\"  '{emotion}': {idx}\")\n",
    "else:\n",
    "    # It's already index->emotion format\n",
    "    print(\"\\nEmotion Labels:\")\n",
    "    for idx, emotion in sorted(label_mapping.items(), key=lambda x: int(x[0])):\n",
    "        print(f\"  {idx}: {emotion}\")\n",
    "\n",
    "print(f\"\\nNumber of emotion classes: {len(label_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (21.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Features Shape: (122017, 22)\n",
      "\n",
      "Columns (22 total):\n",
      "['sent_neg', 'sent_neu', 'sent_pos', 'sent_compound', 'POS_ADJ_norm', 'POS_ADP_norm', 'POS_ADV_norm', 'POS_AUX_norm', 'POS_CCONJ_norm', 'POS_DET_norm', 'POS_INTJ_norm', 'POS_NOUN_norm', 'POS_NUM_norm', 'POS_PART_norm', 'POS_PRON_norm', 'POS_PROPN_norm', 'POS_PUNCT_norm', 'POS_SCONJ_norm', 'POS_SYM_norm', 'POS_VERB_norm', 'POS_X_norm', 'POS_SPACE_norm']\n",
      "\n",
      "First few rows:\n",
      "   sent_neg  sent_neu  sent_pos  sent_compound  POS_ADJ_norm  POS_ADP_norm  \\\n",
      "0     0.144     0.588     0.268         0.2263      0.066667      0.000000   \n",
      "1     0.031     0.769     0.199         0.9945      0.063830      0.042553   \n",
      "2     0.103     0.897     0.000        -0.3182      0.120000      0.080000   \n",
      "3     0.283     0.552     0.166        -0.4019      0.250000      0.083333   \n",
      "4     0.025     0.695     0.280         0.9365      0.019608      0.078431   \n",
      "\n",
      "   POS_ADV_norm  POS_AUX_norm  POS_CCONJ_norm  POS_DET_norm  ...  \\\n",
      "0      0.133333      0.133333        0.000000      0.200000  ...   \n",
      "1      0.031915      0.081560        0.021277      0.081560  ...   \n",
      "2      0.000000      0.120000        0.000000      0.080000  ...   \n",
      "3      0.166667      0.083333        0.000000      0.083333  ...   \n",
      "4      0.078431      0.058824        0.000000      0.078431  ...   \n",
      "\n",
      "   POS_NUM_norm  POS_PART_norm  POS_PRON_norm  POS_PROPN_norm  POS_PUNCT_norm  \\\n",
      "0           0.0       0.000000       0.000000        0.066667        0.200000   \n",
      "1           0.0       0.039007       0.102837        0.039007        0.156028   \n",
      "2           0.0       0.040000       0.080000        0.120000        0.080000   \n",
      "3           0.0       0.000000       0.083333        0.000000        0.166667   \n",
      "4           0.0       0.019608       0.137255        0.078431        0.196078   \n",
      "\n",
      "   POS_SCONJ_norm  POS_SYM_norm  POS_VERB_norm  POS_X_norm  POS_SPACE_norm  \n",
      "0            0.00           0.0       0.066667         0.0        0.000000  \n",
      "1            0.00           0.0       0.102837         0.0        0.067376  \n",
      "2            0.08           0.0       0.080000         0.0        0.000000  \n",
      "3            0.00           0.0       0.000000         0.0        0.000000  \n",
      "4            0.00           0.0       0.078431         0.0        0.058824  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Restart Python kernel's pyarrow if needed\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pandas as pd\n",
    "    # Force reimport to clear any cached registrations\n",
    "    import importlib\n",
    "    importlib.reload(pa)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Load dense features - adjust path\n",
    "from pathlib import Path\n",
    "DENSE_PATH = Path('artifacts/features/dense/')\n",
    "\n",
    "try:\n",
    "    dense_features = pd.read_parquet(DENSE_PATH / 'dense_features_v1.parquet', engine='pyarrow')\n",
    "except Exception as e:\n",
    "    print(f\"PyArrow error, trying fastparquet...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fastparquet\"])\n",
    "        dense_features = pd.read_parquet(DENSE_PATH / 'dense_features_v1.parquet', engine='fastparquet')\n",
    "    except:\n",
    "        print(\"Error loading parquet. Please restart your kernel and try again.\")\n",
    "        raise\n",
    "\n",
    "print(\"Dense Features Shape:\", dense_features.shape)\n",
    "print(f\"\\nColumns ({len(dense_features.columns)} total):\")\n",
    "print(list(dense_features.columns))\n",
    "\n",
    "# Check split distribution\n",
    "if 'split' in dense_features.columns:\n",
    "    print(f\"\\nSplit distribution:\")\n",
    "    print(dense_features['split'].value_counts())\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(dense_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense feature columns (22 features):\n",
      "  1. sent_neg\n",
      "  2. sent_neu\n",
      "  3. sent_pos\n",
      "  4. sent_compound\n",
      "  5. POS_ADJ_norm\n",
      "  6. POS_ADP_norm\n",
      "  7. POS_ADV_norm\n",
      "  8. POS_AUX_norm\n",
      "  9. POS_CCONJ_norm\n",
      "  10. POS_DET_norm\n",
      "  11. POS_INTJ_norm\n",
      "  12. POS_NOUN_norm\n",
      "  13. POS_NUM_norm\n",
      "  14. POS_PART_norm\n",
      "  15. POS_PRON_norm\n",
      "  16. POS_PROPN_norm\n",
      "  17. POS_PUNCT_norm\n",
      "  18. POS_SCONJ_norm\n",
      "  19. POS_SYM_norm\n",
      "  20. POS_VERB_norm\n",
      "  21. POS_X_norm\n",
      "  22. POS_SPACE_norm\n",
      "\n",
      "Scaler type: <class 'sklearn.preprocessing._data.MaxAbsScaler'>\n"
     ]
    }
   ],
   "source": [
    "# Load feature columns list\n",
    "with open(DENSE_PATH / 'dense_feature_columns_v1.json', 'r') as f:\n",
    "    feature_columns = json.load(f)\n",
    "\n",
    "print(f\"Dense feature columns ({len(feature_columns)} features):\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Load scaler\n",
    "scaler = joblib.load(DENSE_PATH / 'dense_scaler_v1.joblib')\n",
    "print(f\"\\nScaler type: {type(scaler)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Consistency Check:\n",
      "✓ Train: sequences=109816, labels=109815, match=False\n",
      "✓ Valid: sequences=6102, labels=6101, match=False\n",
      "✓ Test:  sequences=6102, labels=6101, match=False\n",
      "\n",
      "⚠️ WARNING: Train data mismatch! Sequences=109816, Labels=109815\n",
      "You'll need to trim sequences or check which sample is missing from labels\n",
      "⚠️ WARNING: Valid data mismatch! Sequences=6102, Labels=6101\n",
      "⚠️ WARNING: Test data mismatch! Sequences=6102, Labels=6101\n",
      "\n",
      "==================================================\n",
      "Dense Features Check:\n",
      "Dense features total shape: (122017, 22)\n",
      "Assuming features are in same order as sequences\n",
      "✓ Split dense features: train=109815, valid=6101, test=6101\n"
     ]
    }
   ],
   "source": [
    "# Check if everything matches\n",
    "print(\"Data Consistency Check:\")\n",
    "print(f\"✓ Train: sequences={X_train_seq.shape[0]}, labels={len(y_train)}, match={X_train_seq.shape[0]==len(y_train)}\")\n",
    "print(f\"✓ Valid: sequences={X_valid_seq.shape[0]}, labels={len(y_valid)}, match={X_valid_seq.shape[0]==len(y_valid)}\")\n",
    "print(f\"✓ Test:  sequences={X_test_seq.shape[0]}, labels={len(y_test)}, match={X_test_seq.shape[0]==len(y_test)}\")\n",
    "\n",
    "# WARNING: Sequences have 1 more sample than labels - need to fix this!\n",
    "if X_train_seq.shape[0] != len(y_train):\n",
    "    print(f\"\\n⚠️ WARNING: Train data mismatch! Sequences={X_train_seq.shape[0]}, Labels={len(y_train)}\")\n",
    "    print(\"You'll need to trim sequences or check which sample is missing from labels\")\n",
    "    \n",
    "if X_valid_seq.shape[0] != len(y_valid):\n",
    "    print(f\"⚠️ WARNING: Valid data mismatch! Sequences={X_valid_seq.shape[0]}, Labels={len(y_valid)}\")\n",
    "    \n",
    "if X_test_seq.shape[0] != len(y_test):\n",
    "    print(f\"⚠️ WARNING: Test data mismatch! Sequences={X_test_seq.shape[0]}, Labels={len(y_test)}\")\n",
    "\n",
    "# Check dense features (handle if no split column)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Dense Features Check:\")\n",
    "\n",
    "if 'split' in dense_features.columns:\n",
    "    # Has split column\n",
    "    train_dense = dense_features[dense_features['split'] == 'train']\n",
    "    valid_dense = dense_features[dense_features['split'] == 'valid']\n",
    "    test_dense = dense_features[dense_features['split'] == 'test']\n",
    "    \n",
    "    print(f\"✓ Dense features: train={len(train_dense)}, valid={len(valid_dense)}, test={len(test_dense)}\")\n",
    "    print(f\"✓ Dense match labels: train={len(train_dense)==len(y_train)}, valid={len(valid_dense)==len(y_valid)}, test={len(test_dense)==len(y_test)}\")\n",
    "else:\n",
    "    # No split column - features are already in separate rows\n",
    "    print(f\"Dense features total shape: {dense_features.shape}\")\n",
    "    print(f\"Assuming features are in same order as sequences\")\n",
    "    \n",
    "    # You'll need to split manually based on index\n",
    "    train_end = len(y_train)\n",
    "    valid_end = train_end + len(y_valid)\n",
    "    \n",
    "    train_dense = dense_features.iloc[:train_end]\n",
    "    valid_dense = dense_features.iloc[train_end:valid_end]\n",
    "    test_dense = dense_features.iloc[valid_end:]\n",
    "    \n",
    "    print(f\"✓ Split dense features: train={len(train_dense)}, valid={len(valid_dense)}, test={len(test_dense)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing data mismatch by trimming sequences...\n",
      "After trimming:\n",
      "✓ Train: sequences=109815, labels=109815, match=True\n",
      "✓ Valid: sequences=6101, labels=6101, match=True\n",
      "✓ Test:  sequences=6101, labels=6101, match=True\n",
      "\n",
      "✅ Data is now ready for LSTM training!\n"
     ]
    }
   ],
   "source": [
    "# Since sequences have 1 extra sample, trim them to match labels\n",
    "print(\"Fixing data mismatch by trimming sequences...\")\n",
    "\n",
    "# Trim to match label count\n",
    "X_train_seq = X_train_seq[:len(y_train)]\n",
    "X_valid_seq = X_valid_seq[:len(y_valid)]\n",
    "X_test_seq = X_test_seq[:len(y_test)]\n",
    "\n",
    "print(\"After trimming:\")\n",
    "print(f\"✓ Train: sequences={X_train_seq.shape[0]}, labels={len(y_train)}, match={X_train_seq.shape[0]==len(y_train)}\")\n",
    "print(f\"✓ Valid: sequences={X_valid_seq.shape[0]}, labels={len(y_valid)}, match={X_valid_seq.shape[0]==len(y_valid)}\")\n",
    "print(f\"✓ Test:  sequences={X_test_seq.shape[0]}, labels={len(y_test)}, match={X_test_seq.shape[0]==len(y_test)}\")\n",
    "\n",
    "print(\"\\n✅ Data is now ready for LSTM training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Test Data Shape: (5994, 7)\n",
      "\n",
      "Columns:\n",
      "['Start Time', 'End Time', 'Sentence', 'Translation', 'Emotion_fine', 'Emotion_core', 'Intensity']\n",
      "\n",
      "First few rows:\n",
      "            Start Time             End Time  \\\n",
      "0  1900-01-01 00:00:00  1900-01-01 00:00:02   \n",
      "1  1900-01-01 00:00:02  1900-01-01 00:00:04   \n",
      "2  1900-01-01 00:00:04  1900-01-01 00:00:05   \n",
      "3  1900-01-01 00:00:05  1900-01-01 00:00:07   \n",
      "4  1900-01-01 00:00:07  1900-01-01 00:00:09   \n",
      "\n",
      "                              Sentence  \\\n",
      "0              لا يوجد علاقة بدون حاجة   \n",
      "1                                قاعدة   \n",
      "2                        لا يوجد علاقة   \n",
      "3            حتى العلاقة مع الله لحاجة   \n",
      "4  ولكن علاقة الله مع الناس ليست لحاجة   \n",
      "\n",
      "                                         Translation   Emotion_fine  \\\n",
      "0              There is no relationship without need    resignation   \n",
      "1                                               Base     neutrality   \n",
      "2                           There is no relationship     detachment   \n",
      "3       Even the relationship with God is for a need    resignation   \n",
      "4  But God's relationship with people is not out ...  contemplation   \n",
      "\n",
      "  Emotion_core Intensity  \n",
      "0      neutral   neutral  \n",
      "1      neutral   neutral  \n",
      "2      neutral   neutral  \n",
      "3      sadness      mild  \n",
      "4      neutral   neutral  \n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5994 entries, 0 to 5993\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Start Time    5994 non-null   object\n",
      " 1   End Time      5994 non-null   object\n",
      " 2   Sentence      5994 non-null   object\n",
      " 3   Translation   5994 non-null   object\n",
      " 4   Emotion_fine  5994 non-null   object\n",
      " 5   Emotion_core  5994 non-null   object\n",
      " 6   Intensity     5994 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 327.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the new test data\n",
    "new_test_data = pd.read_csv('group_8_url_1_transcript.csv')\n",
    "\n",
    "print(\"New Test Data Shape:\", new_test_data.shape)\n",
    "print(\"\\nColumns:\")\n",
    "print(new_test_data.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(new_test_data.head())\n",
    "print(\"\\nData info:\")\n",
    "print(new_test_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from each column:\n",
      "\n",
      "\n",
      "Start Time:\n",
      "0    1900-01-01 00:00:00\n",
      "1    1900-01-01 00:00:02\n",
      "2    1900-01-01 00:00:04\n",
      "Name: Start Time, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "End Time:\n",
      "0    1900-01-01 00:00:02\n",
      "1    1900-01-01 00:00:04\n",
      "2    1900-01-01 00:00:05\n",
      "Name: End Time, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Sentence:\n",
      "0    لا يوجد علاقة بدون حاجة\n",
      "1                      قاعدة\n",
      "2              لا يوجد علاقة\n",
      "Name: Sentence, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Translation:\n",
      "0    There is no relationship without need\n",
      "1                                     Base\n",
      "2                 There is no relationship\n",
      "Name: Translation, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Emotion_fine:\n",
      "0    resignation\n",
      "1     neutrality\n",
      "2     detachment\n",
      "Name: Emotion_fine, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Emotion_core:\n",
      "0    neutral\n",
      "1    neutral\n",
      "2    neutral\n",
      "Name: Emotion_core, dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "Intensity:\n",
      "0    neutral\n",
      "1    neutral\n",
      "2    neutral\n",
      "Name: Intensity, dtype: object\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show sample of each column to identify which contains text\n",
    "print(\"Sample data from each column:\\n\")\n",
    "for col in new_test_data.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(new_test_data[col].head(3))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using column: Translation\n",
      "Number of test samples: 5994\n",
      "\n",
      "Sample texts:\n",
      "\n",
      "1. There is no relationship without need\n",
      "\n",
      "2. Base\n",
      "\n",
      "3. There is no relationship\n",
      "\n",
      "4. Even the relationship with God is for a need\n",
      "\n",
      "5. But God's relationship with people is not out of need.\n"
     ]
    }
   ],
   "source": [
    "# You have two text options:\n",
    "# 1. 'Sentence' - Arabic text\n",
    "# 2. 'Translation' - English text\n",
    "\n",
    "# IMPORTANT: Use the same language that was used to train your model!\n",
    "# If your model was trained on English, use 'Translation'\n",
    "# If trained on Arabic, use 'Sentence'\n",
    "\n",
    "TEXT_COLUMN = 'Translation'  # CHANGE THIS based on your training data\n",
    "\n",
    "# Extract text\n",
    "test_texts = new_test_data[TEXT_COLUMN].fillna('').astype(str).tolist()\n",
    "\n",
    "print(f\"Using column: {TEXT_COLUMN}\")\n",
    "print(f\"Number of test samples: {len(test_texts)}\")\n",
    "print(\"\\nSample texts:\")\n",
    "for i, text in enumerate(test_texts[:5]):\n",
    "    print(f\"\\n{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenized 5994 texts\n",
      "\n",
      "Sample tokenized sequence (first 30 tokens):\n",
      "[51, 9, 42, 836, 323, 94]\n",
      "\n",
      "Sequence length statistics:\n",
      "  Min: 1\n",
      "  Max: 29\n",
      "  Mean: 6.11\n",
      "  Median: 6.00\n",
      "\n",
      "Unknown tokens: 0/36600 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Load the trained tokenizer (from your training data)\n",
    "import joblib\n",
    "\n",
    "tokenizer = joblib.load('artifacts/features/sequences/tokenizer_v1.joblib')\n",
    "\n",
    "# Convert texts to sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "print(f\"✓ Tokenized {len(test_sequences)} texts\")\n",
    "print(\"\\nSample tokenized sequence (first 30 tokens):\")\n",
    "print(test_sequences[0][:30])\n",
    "\n",
    "# Check sequence lengths\n",
    "seq_lengths = [len(seq) for seq in test_sequences]\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"  Min: {min(seq_lengths)}\")\n",
    "print(f\"  Max: {max(seq_lengths)}\")\n",
    "print(f\"  Mean: {np.mean(seq_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(seq_lengths):.2f}\")\n",
    "\n",
    "# Check for unknown words (tokens that weren't in training vocab)\n",
    "unknown_counts = [seq.count(0) for seq in test_sequences]\n",
    "total_tokens = sum(seq_lengths)\n",
    "total_unknown = sum(unknown_counts)\n",
    "print(f\"\\nUnknown tokens: {total_unknown}/{total_tokens} ({100*total_unknown/total_tokens:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data max length: 65\n",
      "\n",
      "✓ Padded sequences shape: (5994, 65)\n",
      "✓ Sequence length: 65\n",
      "\n",
      "Sample padded sequence (first 30 tokens):\n",
      "[ 51   9  42 836 323  94   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "\n",
      "Sequences truncated: 0/5994 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Get max length from training data\n",
    "MAX_LENGTH = X_train_seq.shape[1]\n",
    "\n",
    "print(f\"Training data max length: {MAX_LENGTH}\")\n",
    "\n",
    "# Pad sequences\n",
    "X_new_test = pad_sequences(test_sequences, \n",
    "                           maxlen=MAX_LENGTH, \n",
    "                           padding='post',\n",
    "                           truncating='post')\n",
    "\n",
    "print(f\"\\n✓ Padded sequences shape: {X_new_test.shape}\")\n",
    "print(f\"✓ Sequence length: {X_new_test.shape[1]}\")\n",
    "print(\"\\nSample padded sequence (first 30 tokens):\")\n",
    "print(X_new_test[0][:30])\n",
    "\n",
    "# Check how many sequences were truncated\n",
    "truncated = sum(1 for s in test_sequences if len(s) > MAX_LENGTH)\n",
    "print(f\"\\nSequences truncated: {truncated}/{len(test_sequences)} ({100*truncated/len(test_sequences):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth emotions available:\n",
      "\n",
      "Emotion_fine distribution:\n",
      "Emotion_fine\n",
      "curiosity         785\n",
      "neutrality        296\n",
      "resignation       291\n",
      "confusion         265\n",
      "acceptance        173\n",
      "                 ... \n",
      "purposefulness      1\n",
      "virtue              1\n",
      "courtesy            1\n",
      "prohibition         1\n",
      "terror              1\n",
      "Name: count, Length: 441, dtype: int64\n",
      "\n",
      "Emotion_core distribution:\n",
      "Emotion_core\n",
      "neutral      3147\n",
      "happiness     847\n",
      "sadness       841\n",
      "anger         406\n",
      "fear          292\n",
      "surprise      247\n",
      "disgust       214\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Intensity distribution:\n",
      "Intensity\n",
      "mild        2465\n",
      "neutral     2369\n",
      "moderate    1073\n",
      "intense       87\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "From training label mapping:\n",
      "{'anger': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'neutral': 4, 'sadness': 5, 'surprise': 6}\n",
      "\n",
      "⚠️ WARNING: Emotion labels don't match training data!\n",
      "\n",
      "✓ Using emotion column: Emotion_core\n"
     ]
    }
   ],
   "source": [
    "# Your data already has emotion labels!\n",
    "print(\"Ground truth emotions available:\")\n",
    "print(\"\\nEmotion_fine distribution:\")\n",
    "print(new_test_data['Emotion_fine'].value_counts())\n",
    "\n",
    "print(\"\\nEmotion_core distribution:\")\n",
    "print(new_test_data['Emotion_core'].value_counts())\n",
    "\n",
    "print(\"\\nIntensity distribution:\")\n",
    "print(new_test_data['Intensity'].value_counts())\n",
    "\n",
    "# Check which emotion column was used in training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"From training label mapping:\")\n",
    "print(label_mapping)\n",
    "\n",
    "# Determine which emotion column to use\n",
    "if set(label_mapping.values()).intersection(set(new_test_data['Emotion_core'].unique())):\n",
    "    EMOTION_COL = 'Emotion_core'\n",
    "elif set(label_mapping.values()).intersection(set(new_test_data['Emotion_fine'].unique())):\n",
    "    EMOTION_COL = 'Emotion_fine'\n",
    "else:\n",
    "    print(\"\\n⚠️ WARNING: Emotion labels don't match training data!\")\n",
    "    EMOTION_COL = 'Emotion_core'  # default\n",
    "\n",
    "print(f\"\\n✓ Using emotion column: {EMOTION_COL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 5994 labels\n",
      "  Valid labels: 5994\n",
      "  Unknown labels: 0\n",
      "\n",
      "Label distribution:\n",
      "  anger (0): 406\n",
      "  disgust (1): 214\n",
      "  fear (2): 292\n",
      "  happiness (3): 847\n",
      "  neutral (4): 3147\n",
      "  sadness (5): 841\n",
      "  surprise (6): 247\n"
     ]
    }
   ],
   "source": [
    "# Convert emotion labels to numeric using the training label mapping\n",
    "emotion_to_id = label_mapping  # This should be {'emotion': id} format\n",
    "\n",
    "# Get true labels\n",
    "true_emotions = new_test_data[EMOTION_COL].tolist()\n",
    "\n",
    "# Convert to numeric\n",
    "y_new_test = []\n",
    "unknown_emotions = []\n",
    "\n",
    "for emotion in true_emotions:\n",
    "    if emotion in emotion_to_id:\n",
    "        y_new_test.append(emotion_to_id[emotion])\n",
    "    else:\n",
    "        unknown_emotions.append(emotion)\n",
    "        y_new_test.append(-1)  # Mark as unknown\n",
    "\n",
    "y_new_test = np.array(y_new_test)\n",
    "\n",
    "print(f\"✓ Converted {len(y_new_test)} labels\")\n",
    "print(f\"  Valid labels: {sum(y_new_test >= 0)}\")\n",
    "print(f\"  Unknown labels: {sum(y_new_test == -1)}\")\n",
    "\n",
    "if unknown_emotions:\n",
    "    print(f\"\\n⚠️ Unknown emotions found (not in training):\")\n",
    "    print(set(unknown_emotions))\n",
    "    \n",
    "print(f\"\\nLabel distribution:\")\n",
    "unique, counts = np.unique(y_new_test[y_new_test >= 0], return_counts=True)\n",
    "for label_id, count in zip(unique, counts):\n",
    "    # Reverse lookup emotion name\n",
    "    emotion_name = [k for k, v in emotion_to_id.items() if v == label_id][0]\n",
    "    print(f\"  {emotion_name} ({label_id}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARED TEST DATA SUMMARY\n",
      "======================================================================\n",
      "\n",
      "✓ Original data: 5994 samples\n",
      "✓ Text column used: 'Translation'\n",
      "✓ Emotion column used: 'Emotion_core'\n",
      "✓ Tokenized sequences shape: (5994, 65)\n",
      "✓ Sequence length: 65\n",
      "✓ Vocabulary size: 46273\n",
      "✓ Labels shape: (5994,)\n",
      "✓ Number of emotion classes: 7\n",
      "\n",
      "✓ Valid test samples (excluding unknown emotions): 5994\n",
      "\n",
      "======================================================================\n",
      "✅ DATA IS READY FOR PREDICTION!\n",
      "======================================================================\n",
      "\n",
      "To make predictions:\n",
      "  predictions = model.predict(X_new_test_valid)\n",
      "\n",
      "To evaluate:\n",
      "  from sklearn.metrics import classification_report\n",
      "  pred_classes = predictions.argmax(axis=1)\n",
      "  print(classification_report(y_new_test_valid, pred_classes))\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREPARED TEST DATA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n✓ Original data: {new_test_data.shape[0]} samples\")\n",
    "print(f\"✓ Text column used: '{TEXT_COLUMN}'\")\n",
    "print(f\"✓ Emotion column used: '{EMOTION_COL}'\")\n",
    "print(f\"✓ Tokenized sequences shape: {X_new_test.shape}\")\n",
    "print(f\"✓ Sequence length: {MAX_LENGTH}\")\n",
    "print(f\"✓ Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"✓ Labels shape: {y_new_test.shape}\")\n",
    "print(f\"✓ Number of emotion classes: {len(label_mapping)}\")\n",
    "\n",
    "# Filter out unknown labels for evaluation\n",
    "valid_indices = y_new_test >= 0\n",
    "X_new_test_valid = X_new_test[valid_indices]\n",
    "y_new_test_valid = y_new_test[valid_indices]\n",
    "\n",
    "print(f\"\\n✓ Valid test samples (excluding unknown emotions): {len(X_new_test_valid)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ DATA IS READY FOR PREDICTION!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTo make predictions:\")\n",
    "print(\"  predictions = model.predict(X_new_test_valid)\")\n",
    "print(\"\\nTo evaluate:\")\n",
    "print(\"  from sklearn.metrics import classification_report\")\n",
    "print(\"  pred_classes = predictions.argmax(axis=1)\")\n",
    "print(\"  print(classification_report(y_new_test_valid, pred_classes))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared test dataset:\n",
      "            Start Time             End Time  \\\n",
      "0  1900-01-01 00:00:00  1900-01-01 00:00:02   \n",
      "1  1900-01-01 00:00:02  1900-01-01 00:00:04   \n",
      "2  1900-01-01 00:00:04  1900-01-01 00:00:05   \n",
      "3  1900-01-01 00:00:05  1900-01-01 00:00:07   \n",
      "4  1900-01-01 00:00:07  1900-01-01 00:00:09   \n",
      "\n",
      "                              Sentence  \\\n",
      "0              لا يوجد علاقة بدون حاجة   \n",
      "1                                قاعدة   \n",
      "2                        لا يوجد علاقة   \n",
      "3            حتى العلاقة مع الله لحاجة   \n",
      "4  ولكن علاقة الله مع الناس ليست لحاجة   \n",
      "\n",
      "                                         Translation   Emotion_fine  \\\n",
      "0              There is no relationship without need    resignation   \n",
      "1                                               Base     neutrality   \n",
      "2                           There is no relationship     detachment   \n",
      "3       Even the relationship with God is for a need    resignation   \n",
      "4  But God's relationship with people is not out ...  contemplation   \n",
      "\n",
      "  Emotion_core Intensity  is_valid  numeric_label  \n",
      "0      neutral   neutral      True              4  \n",
      "1      neutral   neutral      True              4  \n",
      "2      neutral   neutral      True              4  \n",
      "3      sadness      mild      True              5  \n",
      "4      neutral   neutral      True              4  \n",
      "\n",
      "✅ Saved prepared data:\n",
      "  - test_data_prepared.csv\n",
      "  - X_new_test.npz\n",
      "  - y_new_test.npy\n"
     ]
    }
   ],
   "source": [
    "# Create a clean dataframe with prepared data\n",
    "test_prepared = new_test_data.copy()\n",
    "test_prepared['is_valid'] = valid_indices\n",
    "test_prepared['numeric_label'] = y_new_test\n",
    "\n",
    "print(\"Prepared test dataset:\")\n",
    "print(test_prepared.head())\n",
    "\n",
    "# Save for later use\n",
    "test_prepared.to_csv('test_data_prepared.csv', index=False)\n",
    "np.savez_compressed('X_new_test.npz', X_new_test_valid)\n",
    "np.save('y_new_test.npy', y_new_test_valid)\n",
    "\n",
    "print(\"\\n✅ Saved prepared data:\")\n",
    "print(\"  - test_data_prepared.csv\")\n",
    "print(\"  - X_new_test.npz\")\n",
    "print(\"  - y_new_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
