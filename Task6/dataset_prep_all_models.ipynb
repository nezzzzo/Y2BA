{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56cc8c8d-ad24-4cb1-87f6-393e8d6b28ef",
   "metadata": {},
   "source": [
    "## **Dataset Info**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d20a6-e29c-4a24-a433-3f41ca7e24ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Selection of Datasets**\n",
    "\n",
    "We chose to combine three well-known emotion datasets: **DailyDialog**, **ISEAR**, and **GoEmotions**.  \n",
    "The goal was to create a dataset that reflects natural, real-life emotional expressions—something that aligns with our podcast episode, which focuses on **relationships** and their **application in daily life**.\n",
    "\n",
    "- **DailyDialog**  \n",
    "  Provides everyday conversations in dialogue form.  \n",
    "  This mirrors spoken interactions, making it relevant to podcast-style exchanges.\n",
    "\n",
    "- **ISEAR (International Survey on Emotion Antecedents and Reactions)**  \n",
    "  Contains personal stories where people describe situations that triggered emotions.  \n",
    "  These reflective accounts connect closely to how relationships and daily experiences shape emotions.\n",
    "\n",
    "- **GoEmotions**  \n",
    "  Reddit comments with 27 detailed emotion labels, mapped into 7 main categories.  \n",
    "  This dataset adds modern, informal language similar to how people express themselves online and in casual talks.\n",
    "\n",
    "**Why these three?**  \n",
    "Together, these datasets cover:  \n",
    "- **Natural conversations** (like in a podcast discussion)  \n",
    "- **Personal reflections** (how people feel in relationships and daily life)  \n",
    "- **Modern expressions** (informal, online style, closer to how emotions are shared today)\n",
    "\n",
    "This makes the final dataset **diverse, comprehensive, and highly relevant** to our episode’s theme of exploring emotions in relationships and everyday life.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408963c1-1307-45d7-ad45-ac97abb202bb",
   "metadata": {},
   "source": [
    "## **Data Preparation for Emotion Classification**\n",
    "\n",
    "In this process, we prepared a clean dataset for training an emotion classification model.  \n",
    "We started with three different datasets: **DailyDialog**, **ISEAR**, and **GoEmotions**.  \n",
    "Since each dataset used different formats and emotion categories, we applied a unified cleaning and mapping pipeline.\n",
    "\n",
    "**Steps we followed:**\n",
    "\n",
    "1. **Load the datasets**  \n",
    "   - Read the raw CSV files for each dataset.\n",
    "\n",
    "2. **Parse and clean text**  \n",
    "   - Converted stored strings into proper Python lists or cleaned plain text.  \n",
    "   - Removed unwanted characters, extra spaces, and invisible Unicode symbols.  \n",
    "   - Dropped rows with missing or empty text.\n",
    "\n",
    "3. **Map emotions into 7 categories**  \n",
    "   - Different datasets had different labels (e.g., *joy*, *remorse*, *gratitude*).  \n",
    "   - We mapped all of them into 7 target emotions:  \n",
    "     **happiness, sadness, surprise, anger, disgust, fear, neutral**.  \n",
    "   - This made the datasets consistent and comparable.\n",
    "\n",
    "4. **Filter and align data**  \n",
    "   - Kept only rows with valid single labels.  \n",
    "   - Dropped duplicates (same text + label pairs).  \n",
    "   - Ensured only non-empty cleaned text remained.\n",
    "\n",
    "5. **Combine datasets**  \n",
    "   - Merged DailyDialog, ISEAR, and GoEmotions into one unified DataFrame.  \n",
    "   - Shuffled the data for randomness while keeping reproducibility.\n",
    "\n",
    "6. **Export final dataset**  \n",
    "   - Saved the cleaned dataset to a **TSV file** (`task6_emotions_clean.tsv`).  \n",
    "   - TSV format avoids issues with commas in text.\n",
    "\n",
    "**Result:**  \n",
    "We now have a single, clean dataset with consistent labels across three sources, ready to be used for training an emotion classification model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d353be9-115e-4d6f-ae40-85159636c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3d5f0d-94a4-4cdc-ac2f-a534c07f6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv('daily_dialog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7175d6da-109f-4360-a87b-ad7b519ca123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11118, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>act</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Say , Jim , how about going for a few beers ...</td>\n",
       "      <td>[3 4 2 2 2 3 4 1 3 4]</td>\n",
       "      <td>[0 0 0 0 0 0 4 4 4 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Can you do push-ups ? '\\n \" Of course I can ...</td>\n",
       "      <td>[2 1 2 2 1 1]</td>\n",
       "      <td>[0 0 6 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Can you study with the radio on ? '\\n ' No ,...</td>\n",
       "      <td>[2 1 2 1 1]</td>\n",
       "      <td>[0 0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Are you all right ? '\\n ' I will be all righ...</td>\n",
       "      <td>[2 1 1 1]</td>\n",
       "      <td>[0 0 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Hey John , nice skates . Are they new ? '\\n ...</td>\n",
       "      <td>[2 1 2 1 1 2 1 3 4]</td>\n",
       "      <td>[0 0 0 0 0 6 0 6 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dialog                    act  \\\n",
       "0  ['Say , Jim , how about going for a few beers ...  [3 4 2 2 2 3 4 1 3 4]   \n",
       "1  ['Can you do push-ups ? '\\n \" Of course I can ...          [2 1 2 2 1 1]   \n",
       "2  ['Can you study with the radio on ? '\\n ' No ,...            [2 1 2 1 1]   \n",
       "3  ['Are you all right ? '\\n ' I will be all righ...              [2 1 1 1]   \n",
       "4  ['Hey John , nice skates . Are they new ? '\\n ...    [2 1 2 1 1 2 1 3 4]   \n",
       "\n",
       "                 emotion  \n",
       "0  [0 0 0 0 0 0 4 4 4 4]  \n",
       "1          [0 0 6 0 0 0]  \n",
       "2            [0 0 0 0 0]  \n",
       "3              [0 0 0 0]  \n",
       "4    [0 0 0 0 0 6 0 6 0]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dd.shape)\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187c9c88-e8a7-4d16-a796-60f451e007f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map numerical emotion labels to their string categories\n",
    "dd_map = {\n",
    "    0: \"neutral\", 1: \"anger\", 2: \"disgust\", 3: \"fear\",\n",
    "    4: \"happiness\", 5: \"sadness\", 6: \"surprise\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42171b4f-e13f-4627-adac-6699356a2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Convert \"dialog\" strings into Python lists (fallback: wrap as single-item list)\n",
    "dd[\"dialog\"] = dd[\"dialog\"].apply(lambda x: ast.literal_eval(str(x)) if str(x).startswith(\"[\") else [str(x)])\n",
    "\n",
    "# Convert \"emotion\" strings like \"[3 4 2]\" into lists of integers\n",
    "dd[\"emotion\"] = dd[\"emotion\"].apply(\n",
    "    lambda x: [int(n) for n in re.split(r\"[ ,]+\", str(x).strip(\"[]\")) if n.isdigit()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570662ab-996d-452a-8395-60b45b818b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rows = []\n",
    "\n",
    "# Iterate through each row of the dataset\n",
    "for _, row in dd.iterrows():\n",
    "    dialogs = row[\"dialog\"] \n",
    "    emos = row[\"emotion\"]  \n",
    "    \n",
    "    # Ensure both lists are aligned by taking the shorter length\n",
    "    L = min(len(dialogs), len(emos))\n",
    "    \n",
    "    # Pair each utterance with its corresponding emotion\n",
    "    for i in range(L):\n",
    "        text_i = str(dialogs[i]).strip()\n",
    "        emo_i  = dd_map.get(int(emos[i]), None)\n",
    "        \n",
    "        # Keep only non-empty utterances with valid emotions\n",
    "        if text_i and emo_i in {\"happiness\",\"sadness\",\"surprise\",\"anger\",\"disgust\",\"fear\",\"neutral\"}:\n",
    "            dd_rows.append({\"text\": text_i, \"label\": emo_i, \"source\": \"dailydialog\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b501336-b706-4336-ae81-33e34be7d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11118, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Say , Jim , how about going for a few beers af...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dailydialog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you do push-ups ?  Of course I can . It's ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dailydialog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you study with the radio on ?  No , I list...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dailydialog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are you all right ?  I will be all right soon ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dailydialog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey John , nice skates . Are they new ?  Yeah ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dailydialog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label       source\n",
       "0  Say , Jim , how about going for a few beers af...  neutral  dailydialog\n",
       "1  Can you do push-ups ?  Of course I can . It's ...  neutral  dailydialog\n",
       "2  Can you study with the radio on ?  No , I list...  neutral  dailydialog\n",
       "3  Are you all right ?  I will be all right soon ...  neutral  dailydialog\n",
       "4  Hey John , nice skates . Are they new ?  Yeah ...  neutral  dailydialog"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_clean = pd.DataFrame(dd_rows)\n",
    "# Quick inspection\n",
    "print(dd_clean.shape)\n",
    "dd_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b79c75d-d3b4-43b9-b711-478138b42d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "neutral      9786\n",
       "happiness     929\n",
       "anger         136\n",
       "surprise       96\n",
       "sadness        92\n",
       "disgust        56\n",
       "fear           23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the classes distribution\n",
    "dd_clean['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8874fe9c-cfe7-4659-9994-e5df71a56e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "isear = pd.read_csv('ISEAR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1eb61b3-d4b8-4513-8ad3-2225f64924e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7102, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10941</td>\n",
       "      <td>anger</td>\n",
       "      <td>At the point today where if someone says somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10942</td>\n",
       "      <td>anger</td>\n",
       "      <td>@CorningFootball  IT'S GAME DAY!!!!      T MIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10943</td>\n",
       "      <td>anger</td>\n",
       "      <td>This game has pissed me off more than any othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10944</td>\n",
       "      <td>anger</td>\n",
       "      <td>@spamvicious I've just found out it's Candice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10945</td>\n",
       "      <td>anger</td>\n",
       "      <td>@moocowward @mrsajhargreaves @Melly77 @GaryBar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID sentiment                                            content\n",
       "0  10941     anger  At the point today where if someone says somet...\n",
       "1  10942     anger  @CorningFootball  IT'S GAME DAY!!!!      T MIN...\n",
       "2  10943     anger  This game has pissed me off more than any othe...\n",
       "3  10944     anger  @spamvicious I've just found out it's Candice ...\n",
       "4  10945     anger  @moocowward @mrsajhargreaves @Melly77 @GaryBar..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(isear.shape)\n",
    "isear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b2353a9-3a7b-4a9f-b56d-9ca22e689f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ISEAR dataset emotions to unified categories\n",
    "isear_map = {\n",
    "    \"anger\":\"anger\",\n",
    "    \"disgust\":\"disgust\",\n",
    "    \"fear\":\"fear\",\n",
    "    \"joy\":\"happiness\",\n",
    "    \"happiness\":\"happiness\",\n",
    "    \"sadness\":\"sadness\",\n",
    "    \"shame\":\"sadness\",   \n",
    "    \"guilt\":\"sadness\",  \n",
    "    \"neutral\":\"neutral\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa0c58e9-e4e1-4ec8-8853-d43be98b7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize sentiment values: lowercase + remove extra spaces\n",
    "isear[\"sentiment_norm\"] = isear[\"sentiment\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Map normalized sentiments to unified emotion labels\n",
    "isear[\"label\"] = isear[\"sentiment_norm\"].map(isear_map)\n",
    "\n",
    "# Clean up content text (ensure string, strip whitespace)\n",
    "isear[\"text\"] = isear[\"content\"].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82511f78-7352-4f33-b370-059928b0385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the mapped label is missing\n",
    "isear_clean = isear.dropna(subset=[\"label\"])\n",
    "\n",
    "# Keep only rows with non-empty text, selecting text + label columns\n",
    "isear_clean = isear_clean.loc[isear_clean[\"text\"].str.len() > 0, [\"text\",\"label\"]].copy()\n",
    "\n",
    "# Add a source column to identify dataset origin\n",
    "isear_clean[\"source\"] = \"isear\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6ec1ce6-9142-4c37-8c72-d079a6842f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - remove leading '>' markers\n",
    "# - replace invisible/Unicode spaces with normal spaces\n",
    "# - collapse multiple spaces into one\n",
    "# - strip leading/trailing spaces\n",
    "isear_clean[\"text\"] = (isear_clean[\"text\"]\n",
    "    .str.replace(r\"^\\s*>\\s*\", \"\", regex=True)\n",
    "    .str.replace(\"\\u200b\",\" \")\n",
    "    .str.replace(\"\\xa0\",\" \")\n",
    "    .str.replace(r\"\\s+\",\" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Drop duplicate rows (same text + label) and reset index\n",
    "isear_clean = isear_clean.drop_duplicates(subset=[\"text\",\"label\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22af728d-a7e6-40bb-8c64-e0ce1501ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7101, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the point today where if someone says somet...</td>\n",
       "      <td>anger</td>\n",
       "      <td>isear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@CorningFootball IT'S GAME DAY!!!! T MINUS 14:...</td>\n",
       "      <td>anger</td>\n",
       "      <td>isear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This game has pissed me off more than any othe...</td>\n",
       "      <td>anger</td>\n",
       "      <td>isear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@spamvicious I've just found out it's Candice ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>isear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@moocowward @mrsajhargreaves @Melly77 @GaryBar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>isear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label source\n",
       "0  At the point today where if someone says somet...  anger  isear\n",
       "1  @CorningFootball IT'S GAME DAY!!!! T MINUS 14:...  anger  isear\n",
       "2  This game has pissed me off more than any othe...  anger  isear\n",
       "3  @spamvicious I've just found out it's Candice ...  anger  isear\n",
       "4  @moocowward @mrsajhargreaves @Melly77 @GaryBar...  anger  isear"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick inspiction\n",
    "print(isear_clean.shape)\n",
    "isear_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "091eb06e-fcce-4102-81f4-eff396cda016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three GoEmotions dataset splits\n",
    "goe1 = pd.read_csv('goemotions_1.csv')\n",
    "goe2 = pd.read_csv('goemotions_2.csv')\n",
    "goe3 = pd.read_csv('goemotions_3.csv')\n",
    "\n",
    "# Combine them into one DataFrame\n",
    "go = pd.concat([goe1, goe2, goe3], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "472b9f0b-0ef4-4e2c-ad64-1b204c45b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211225, 37)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>t3_ajis4z</td>\n",
       "      <td>t1_eew18eq</td>\n",
       "      <td>1.548381e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
       "      <td>eemcysk</td>\n",
       "      <td>TheGreen888</td>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>t3_ai4q37</td>\n",
       "      <td>1.548084e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>t3_abru74</td>\n",
       "      <td>t1_ed2m7g7</td>\n",
       "      <td>1.546428e+09</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>t3_ahulml</td>\n",
       "      <td>1.547965e+09</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>t3_ackt2f</td>\n",
       "      <td>t1_eda65q2</td>\n",
       "      <td>1.546669e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  \\\n",
       "0                                    That game hurt.  eew5j0j   \n",
       "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
       "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                 Man I love reddit.  eeibobj   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "\n",
       "                author            subreddit    link_id   parent_id  \\\n",
       "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
       "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
       "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
       "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
       "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
       "\n",
       "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
       "0  1.548381e+09         1                 False           0  ...     0   \n",
       "1  1.548084e+09        37                  True           0  ...     0   \n",
       "2  1.546428e+09        37                 False           0  ...     0   \n",
       "3  1.547965e+09        18                 False           0  ...     1   \n",
       "4  1.546669e+09         2                 False           0  ...     0   \n",
       "\n",
       "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0            0         0      0            0       0        0        1   \n",
       "1            0         0      0            0       0        0        0   \n",
       "2            0         0      0            0       0        0        0   \n",
       "3            0         0      0            0       0        0        0   \n",
       "4            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        1  \n",
       "3         0        0  \n",
       "4         0        1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(go.shape)\n",
    "go.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca9ecfe8-ef31-472c-8864-5f63836a966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata columns that are not emotion labels\n",
    "meta_cols = {\n",
    "    \"text\",\"id\",\"author\",\"subreddit\",\"link_id\",\n",
    "    \"parent_id\",\"created_utc\",\"rater_id\",\"example_very_unclear\"\n",
    "}\n",
    "\n",
    "# Select candidate label columns (everything except metadata)\n",
    "cand_cols = [c for c in go.columns if c not in meta_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a184c6db-6d6c-4ac3-a7cf-1ea9b2b29520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize label columns so all are numeric (0/1)\n",
    "for c in cand_cols:\n",
    "    s = go[c]\n",
    "    if s.dtype == bool:\n",
    "        # Convert boolean → int (True=1, False=0)\n",
    "        go[c] = s.astype(int)\n",
    "    elif s.dtype == object:\n",
    "        # Clean string values and map to 0/1\n",
    "        go[c] = s.astype(str).str.strip().replace(\n",
    "            {\"True\": \"1\", \"False\": \"0\", \"true\": \"1\", \"false\": \"0\"}\n",
    "        )\n",
    "        # Convert to numeric, invalid entries → NaN\n",
    "        go[c] = pd.to_numeric(go[c], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90815d1c-2a1e-4557-bcc4-73583a9f6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_cols = []\n",
    "\n",
    "# Collect valid emotion label columns (binary: 0/1 only)\n",
    "for c in cand_cols:\n",
    "    vals = go[c].dropna().unique()\n",
    "    if len(vals) > 0 and set(np.unique(vals)).issubset({0,1}):\n",
    "        emo_cols.append(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "258c5ade-4175-4a99-8e31-55ada6f29f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected emotion columns: 28\n",
      "['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear']\n"
     ]
    }
   ],
   "source": [
    "# Show how many emotion label columns were detected and preview the first 15\n",
    "print(\"Detected emotion columns:\", len(emo_cols))\n",
    "print(emo_cols[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "700af25b-280f-4e30-9127-420624058687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map GoEmotions fine-grained labels to 7 broad categories\n",
    "go_to_seven = {\n",
    "    # happiness\n",
    "    \"admiration\":\"happiness\",\"amusement\":\"happiness\",\"approval\":\"happiness\",\n",
    "    \"caring\":\"happiness\",\"desire\":\"happiness\",\"excitement\":\"happiness\",\n",
    "    \"gratitude\":\"happiness\",\"joy\":\"happiness\",\"love\":\"happiness\",\n",
    "    \"optimism\":\"happiness\",\"pride\":\"happiness\",\"relief\":\"happiness\",\"contentment\":\"happiness\",\n",
    "\n",
    "    # anger\n",
    "    \"anger\":\"anger\",\"annoyance\":\"anger\",\"disapproval\":\"anger\",\n",
    "\n",
    "    # disgust\n",
    "    \"disgust\":\"disgust\",\n",
    "\n",
    "    # fear\n",
    "    \"fear\":\"fear\",\"nervousness\":\"fear\",\n",
    "\n",
    "    # sadness\n",
    "    \"sadness\":\"sadness\",\"disappointment\":\"sadness\",\"grief\":\"sadness\",\n",
    "    \"remorse\":\"sadness\",\"embarrassment\":\"sadness\",\n",
    "\n",
    "    # surprise\n",
    "    \"surprise\":\"surprise\",\"realization\":\"surprise\",\"confusion\":\"surprise\",\"curiosity\":\"surprise\",\n",
    "\n",
    "    # neutral\n",
    "    \"neutral\":\"neutral\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c72586-d98a-4ffa-8a47-e7be59efb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only emotion columns that can be mapped to 7-class schema\n",
    "mapped_cols = [c for c in emo_cols if c in go_to_seven]\n",
    "\n",
    "# Subset DataFrame: text + mapped emotion columns\n",
    "go_sub = go[[\"text\"] + mapped_cols].copy()\n",
    "\n",
    "# Clean text: ensure string type and strip whitespace\n",
    "go_sub[\"text\"] = go_sub[\"text\"].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "364c2839-7c46-4ff3-a337-6fd09a209905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - For each row, find all active emotions (columns with value=1)\n",
    "# - Map them to broad categories using go_to_seven\n",
    "# - If exactly one unique category → keep it, else mark as NaN\n",
    "go_sub[\"label\"] = go_sub[mapped_cols].apply(\n",
    "    lambda r: (lambda mapped: mapped[0] if len(mapped) == 1 else np.nan)(\n",
    "        sorted({go_to_seven[c] for c in r.index if r[c] == 1})\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only rows with valid label and non-empty text\n",
    "go_clean = go_sub.dropna(subset=[\"label\"]).loc[go_sub[\"text\"].str.len() > 0, [\"text\",\"label\"]].copy()\n",
    "\n",
    "# Add source column to track dataset origin\n",
    "go_clean[\"source\"] = \"goemotions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a5f2e04-e92a-4aad-9133-d820536eb375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - remove leading '>' markers\n",
    "# - replace invisible/Unicode spaces with normal spaces\n",
    "# - collapse multiple spaces into one\n",
    "# - strip leading/trailing spaces\n",
    "go_clean[\"text\"] = (go_clean[\"text\"]\n",
    "    .str.replace(r\"^\\s*>\\s*\", \"\", regex=True)\n",
    "    .str.replace(\"\\u200b\",\" \")\n",
    "    .str.replace(\"\\xa0\",\" \")\n",
    "    .str.replace(r\"\\s+\",\" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Drop duplicate (text, label) pairs and reset index\n",
    "go_clean = go_clean.drop_duplicates(subset=[\"text\",\"label\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14c70db4-11b1-4115-9a77-ec5f62e0c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104310, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>goemotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>goemotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>goemotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>goemotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Right? Considering it’s such an important docu...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>goemotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label      source\n",
       "0                                    That game hurt.    sadness  goemotions\n",
       "1     You do right, if you don't care then fuck 'em!    neutral  goemotions\n",
       "2                                 Man I love reddit.  happiness  goemotions\n",
       "3  [NAME] was nowhere near them, he was by the Fa...    neutral  goemotions\n",
       "4  Right? Considering it’s such an important docu...  happiness  goemotions"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick inspiction\n",
    "print(go_clean.shape)\n",
    "go_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51d4f89d-be56-4834-aa04-dbce0aa8f29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "happiness    32322\n",
      "neutral      31445\n",
      "anger        14613\n",
      "surprise     12926\n",
      "sadness       8523\n",
      "disgust       2363\n",
      "fear          2118\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check classes didstribution\n",
    "print(go_clean[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b618dccb-562e-44aa-bc73-93375daf166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine cleaned datasets (DailyDialog, GoEmotions, ISEAR) into one DataFrame\n",
    "frames = [dd_clean, go_clean, isear_clean]\n",
    "all_df = pd.concat(frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a12d8e98-3d92-40ce-97a3-852a9ce687cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where label is in the 7 target emotion categories\n",
    "target_set = {\"happiness\",\"sadness\",\"surprise\",\"anger\",\"disgust\",\"fear\",\"neutral\"}\n",
    "all_df = all_df[all_df[\"label\"].isin(target_set)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c60a719-9a76-428b-934a-b3737545b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing text/label\n",
    "all_df = all_df.dropna(subset=[\"text\",\"label\"])\n",
    "\n",
    "# Keep only rows with non-empty text\n",
    "all_df = all_df.loc[all_df[\"text\"].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6764f79f-30f3-4ada-aa49-b729e058a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate (text, label) pairs and reset index\n",
    "all_df = all_df.drop_duplicates(subset=[\"text\",\"label\"]).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the dataset randomly (reproducible with random_state)\n",
    "all_df = all_df.sample(frac=1.0, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd97be79-792c-47b3-984a-8757505e00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a TSV file (tab-separated, no index column)\n",
    "all_out = \"task6_emotions_clean.tsv\"\n",
    "all_df.to_csv(all_out, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9523331f-1cfa-4b90-993e-5685ce8ae917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122017, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yep. This sub is just being overly reactionary...</td>\n",
       "      <td>anger</td>\n",
       "      <td>goemotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do Mona and Jim need a new house ?  No , they ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dailydialog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While we watch the private form of Operation C...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>goemotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is though. Certainly the worst Zelda game a...</td>\n",
       "      <td>anger</td>\n",
       "      <td>goemotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Everybody , I'd like to propose a toast to Mar...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>dailydialog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label       source\n",
       "0  Yep. This sub is just being overly reactionary...      anger   goemotions\n",
       "1  Do Mona and Jim need a new house ?  No , they ...    neutral  dailydialog\n",
       "2  While we watch the private form of Operation C...   surprise   goemotions\n",
       "3  It is though. Certainly the worst Zelda game a...      anger   goemotions\n",
       "4  Everybody , I'd like to propose a toast to Mar...  happiness  dailydialog"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final chekc\n",
    "print(all_df.shape)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08265f46-afa5-4224-944e-bd7917a1a156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "neutral      40768\n",
       "happiness    34835\n",
       "anger        16444\n",
       "surprise     13018\n",
       "sadness      10144\n",
       "fear          4392\n",
       "disgust       2416\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check classes distiribution\n",
    "all_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12813bc-c8bf-487d-a8f3-5c7251eca0f6",
   "metadata": {},
   "source": [
    "## **feature extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa208f8-c809-4deb-bbcc-79067577c26f",
   "metadata": {},
   "source": [
    "### **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff42985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path to raw dataset CSV \n",
    "DATA_CSV = 'task6_emotions_clean.tsv' \n",
    "\n",
    "# Output directory for all artifacts\n",
    "ARTIFACT_DIR = Path(\"artifacts\")\n",
    "ARTIFACT_VERSION = \"v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9a038cd-81da-44ae-8089-8896a892b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ratios\n",
    "TEST_SIZE = 0.10   # 10% test\n",
    "VALID_SIZE = 0.10  # 10% validation\n",
    "\n",
    "# Random seed\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "347795a1-74ef-4adb-941b-e6da57f7ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping \n",
    "LABEL_ORDER = [\"anger\",\"disgust\",\"fear\",\"happiness\",\"neutral\",\"sadness\",\"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "662d8ec0-fb8b-4413-b32d-450cfcdfa419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF params\n",
    "TFIDF_NGRAM_RANGE = (1, 2)\n",
    "TFIDF_MIN_DF = 5\n",
    "TFIDF_MAX_DF = 0.8\n",
    "TFIDF_MAX_FEATURES = 200_000\n",
    "TFIDF_SUBLINEAR_TF = True\n",
    "TFIDF_NORM = \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd9297b3-5e15-463f-b066-a4de1636a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM/RNN tokenization\n",
    "VOCAB_SIZE = 40_000        \n",
    "MAX_LEN = None            \n",
    "MAX_LEN_PERCENTILE = 95    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32f36ddc-acb7-4175-8e59-0463181b61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model\n",
    "HF_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "TRANSFORMER_MAX_LENGTH = 128               # common good default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29d9e42e-415d-44d5-9fd1-a8a4d26043ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense features to include (by prefix)\n",
    "INCLUDE_SENTIMENT = True\n",
    "INCLUDE_POS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8703420-6bb3-4c84-9d86-e1968830159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts will be saved under: /home/y2a/BlockA2/Task6Test/artifacts\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Artifacts will be saved under:\", ARTIFACT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efcdc02",
   "metadata": {},
   "source": [
    "### **Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa4a6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text      label       source\n",
      "0  Yep. This sub is just being overly reactionary...      anger   goemotions\n",
      "1  Do Mona and Jim need a new house ?  No , they ...    neutral  dailydialog\n",
      "2  While we watch the private form of Operation C...   surprise   goemotions\n",
      "3  It is though. Certainly the worst Zelda game a...      anger   goemotions\n",
      "4  Everybody , I'd like to propose a toast to Mar...  happiness  dailydialog\n",
      "Rows: 122017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "all_df = pd.read_csv(DATA_CSV, sep='\\t')\n",
    "assert \"text\" in all_df.columns and \"label\" in all_df.columns, \"Data must include 'text' and 'label' columns.\"\n",
    "print(all_df.head())\n",
    "print(\"Rows:\", len(all_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27259f9d",
   "metadata": {},
   "source": [
    "### **Create stratified train/valid/test splits and save indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e51a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# First: test split\n",
    "X_train_text, X_temp_text, y_train, y_temp = train_test_split(\n",
    "    all_df[\"text\"].astype(str),\n",
    "    all_df[\"label\"],\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=all_df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2beeb41d-5aec-4819-a3b3-005db925a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second: validation split from temp\n",
    "# proportion within the remaining\n",
    "valid_size_rel = VALID_SIZE / (1 - TEST_SIZE)\n",
    "X_valid_text, X_test_text, y_valid, y_test = train_test_split(\n",
    "    X_temp_text,\n",
    "    y_temp,\n",
    "    # split temp into equal valid/test parts to match target ratios\n",
    "    test_size=0.5,  \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1e8e787-1382-4f0c-a8eb-1bc0aa00402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 109815 6101 6101\n"
     ]
    }
   ],
   "source": [
    "# Save indices\n",
    "splits_dir = ARTIFACT_DIR / \"splits\"\n",
    "splits_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_idx = X_train_text.index\n",
    "valid_idx = X_valid_text.index\n",
    "test_idx  = X_test_text.index\n",
    "\n",
    "train_idx.to_series(index=None).to_csv(splits_dir / f\"train_idx_{ARTIFACT_VERSION}.csv\", index=False)\n",
    "valid_idx.to_series(index=None).to_csv(splits_dir / f\"valid_idx_{ARTIFACT_VERSION}.csv\", index=False)\n",
    "test_idx.to_series(index=None).to_csv(splits_dir / f\"test_idx_{ARTIFACT_VERSION}.csv\", index=False)\n",
    "\n",
    "print(\"Split sizes:\", len(train_idx), len(valid_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f7796",
   "metadata": {},
   "source": [
    "### **Create and save a stable label mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9d35b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create dictionaries to map labels to IDs and IDs back to labels\n",
    "label_to_id = {lab: i for i, lab in enumerate(LABEL_ORDER)}\n",
    "id_to_label = {i: lab for lab, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2898abf-c77e-4dd3-bc80-858ed91edf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all labels in the dataset are included in LABEL_ORDER\n",
    "data_labels = set(all_df[\"label\"].unique())\n",
    "missing = data_labels.difference(set(LABEL_ORDER))\n",
    "if missing:\n",
    "    print(\"WARNING: The following labels are in data but not in LABEL_ORDER:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46984541-aefd-4e05-8981-a3ce79c23f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the labels directory if it does not already exist\n",
    "labels_dir = ARTIFACT_DIR / \"labels\"\n",
    "labels_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22f6deae-9b23-4235-8f53-5f8be5fd7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the label-to-ID mapping as a JSON file\n",
    "with open(labels_dir / f\"label_mapping_{ARTIFACT_VERSION}.json\", \"w\") as f:\n",
    "    json.dump(label_to_id, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fb33f46-28b0-4ba6-99ba-ff71ffb94c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved label mapping: {'anger': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'neutral': 4, 'sadness': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the mapping was saved\n",
    "print(\"Saved label mapping:\", label_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25532192",
   "metadata": {},
   "source": [
    "### **Sentiment features (VADER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3356eed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sent_neg  sent_neu  sent_pos  sent_compound\n",
      "0     0.144     0.588     0.268         0.2263\n",
      "1     0.031     0.769     0.199         0.9945\n",
      "2     0.103     0.897     0.000        -0.3182\n",
      "3     0.283     0.552     0.166        -0.4019\n",
      "4     0.025     0.695     0.280         0.9365\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Add sentiment features if enabled\n",
    "if INCLUDE_SENTIMENT:\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Ensure text column has no missing values and is of string type\n",
    "    all_df[\"text\"] = all_df[\"text\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Function to calculate VADER sentiment scores for a given text\n",
    "    def vader_scores(text):\n",
    "        s = analyzer.polarity_scores(text)\n",
    "        return pd.Series(\n",
    "            [s[\"neg\"], s[\"neu\"], s[\"pos\"], s[\"compound\"]],\n",
    "            index=[\"sent_neg\", \"sent_neu\", \"sent_pos\", \"sent_compound\"]\n",
    "        )\n",
    "\n",
    "    # Apply the sentiment scoring function to the text column\n",
    "    sent_df = all_df[\"text\"].apply(vader_scores)\n",
    "    print(sent_df.head())\n",
    "\n",
    "else:\n",
    "    # If sentiment analysis is disabled, create an empty DataFrame\n",
    "    sent_df = pd.DataFrame(index=all_df.index)\n",
    "    print(\"Sentiment features skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e6f74",
   "metadata": {},
   "source": [
    "### **POS features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81ac916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Universal Part-of-Speech tags (UPOS) to track\n",
    "UPOS = [\"ADJ\",\"ADP\",\"ADV\",\"AUX\",\"CCONJ\",\"DET\",\"INTJ\",\"NOUN\",\"NUM\",\n",
    "        \"PART\",\"PRON\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"SYM\",\"VERB\",\"X\",\"SPACE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c1190ec-d66a-4962-ae28-ed5a5ebaeb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract POS tags from a given cell (handles lists of tokens or string formats)\n",
    "def extract_tags_from_cell(cell):\n",
    "    tags = []\n",
    "    if isinstance(cell, list):\n",
    "        for item in cell:\n",
    "            if hasattr(item, \"pos_\"):  # spaCy token\n",
    "                tags.append(item.pos_)\n",
    "            else:  # fallback to regex if not a spaCy token\n",
    "                s = str(item)\n",
    "                m = re.search(r\"/([A-Z]+)\", s)\n",
    "                if m:\n",
    "                    tags.append(m.group(1))\n",
    "    else:\n",
    "        for token in re.findall(r\"/([A-Z]+)\", str(cell)):\n",
    "            tags.append(token)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2a9d5b1-2715-498a-956c-5d60cd808a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count POS tags and calculate normalized ratios\n",
    "def pos_counts_and_ratios(tags):\n",
    "    c = Counter(tags)\n",
    "    counts = {f\"POS_{t}\": c.get(t, 0) for t in UPOS}\n",
    "    total = sum(counts.values())\n",
    "    ratios = {\n",
    "        f\"POS_{t}_norm\": (counts[f\"POS_{t}\"]/total if total > 0 else 0.0)\n",
    "        for t in UPOS\n",
    "    }\n",
    "    return {**counts, **ratios, \"POS_token_total\": total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00ca0f7f-dd3a-41cc-b001-810e05d55ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed POS with spaCy.\n",
      "   POS_ADJ  POS_ADP  POS_ADV  POS_AUX  POS_CCONJ  POS_DET  POS_INTJ  POS_NOUN  \\\n",
      "0        1        0        2        2          0        3         0         2   \n",
      "1       18       12        9       23          6       23         6        42   \n",
      "2        3        2        0        3          0        2         0         3   \n",
      "3        3        1        2        1          0        1         0         1   \n",
      "4        1        4        4        3          0        4         1         5   \n",
      "\n",
      "   POS_NUM  POS_PART  ...  POS_PART_norm  POS_PRON_norm  POS_PROPN_norm  \\\n",
      "0        0         0  ...       0.000000       0.000000        0.066667   \n",
      "1        0        11  ...       0.039007       0.102837        0.039007   \n",
      "2        0         1  ...       0.040000       0.080000        0.120000   \n",
      "3        0         0  ...       0.000000       0.083333        0.000000   \n",
      "4        0         1  ...       0.019608       0.137255        0.078431   \n",
      "\n",
      "   POS_PUNCT_norm  POS_SCONJ_norm  POS_SYM_norm  POS_VERB_norm  POS_X_norm  \\\n",
      "0        0.200000            0.00           0.0       0.066667         0.0   \n",
      "1        0.156028            0.00           0.0       0.102837         0.0   \n",
      "2        0.080000            0.08           0.0       0.080000         0.0   \n",
      "3        0.166667            0.00           0.0       0.000000         0.0   \n",
      "4        0.196078            0.00           0.0       0.078431         0.0   \n",
      "\n",
      "   POS_SPACE_norm  POS_token_total  \n",
      "0        0.000000               15  \n",
      "1        0.067376              282  \n",
      "2        0.000000               25  \n",
      "3        0.000000               12  \n",
      "4        0.058824               51  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add POS features if enabled\n",
    "if INCLUDE_POS:\n",
    "    if \"POS_Tags\" in all_df.columns:\n",
    "        # Use existing POS_Tags column\n",
    "        tags_series = all_df[\"POS_Tags\"]\n",
    "        pos_features = tags_series.apply(\n",
    "            lambda cell: pos_counts_and_ratios(extract_tags_from_cell(cell))\n",
    "        )\n",
    "        pos_df = pd.DataFrame(list(pos_features)).fillna(0)\n",
    "        print(\"Parsed POS from existing 'POS_Tags' column.\")\n",
    "    else:\n",
    "        # Compute POS features with spaCy if not already available\n",
    "        import spacy\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\",\"parser\"])\n",
    "        texts = all_df[\"text\"].fillna(\"\").astype(str).tolist()\n",
    "        feats = []\n",
    "        for doc in nlp.pipe(texts, batch_size=1000, n_process=-1):\n",
    "            counts = doc.count_by(spacy.attrs.POS)\n",
    "            features = {}\n",
    "            for pos_id, count in counts.items():\n",
    "                features[nlp.vocab[pos_id].text] = count\n",
    "\n",
    "            # Align to UPOS and compute normalized ratios\n",
    "            counts_aligned = {f\"POS_{t}\": features.get(t, 0) for t in UPOS}\n",
    "            total = sum(counts_aligned.values())\n",
    "            ratios = {\n",
    "                f\"POS_{t}_norm\": (counts_aligned[f\"POS_{t}\"]/total if total > 0 else 0.0)\n",
    "                for t in UPOS\n",
    "            }\n",
    "            feats.append({**counts_aligned, **ratios, \"POS_token_total\": total})\n",
    "        pos_df = pd.DataFrame(feats).fillna(0)\n",
    "        print(\"Computed POS with spaCy.\")\n",
    "else:\n",
    "    # If POS is disabled, create an empty DataFrame with the right index\n",
    "    pos_df = pd.DataFrame(index=all_df.index)\n",
    "    print(\"POS features skipped.\")\n",
    "\n",
    "# Show first rows of the POS features\n",
    "print(pos_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330a42b",
   "metadata": {},
   "source": [
    "### **Save dense features (sentiment + POS ratios)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc9d69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "dense_dir = ARTIFACT_DIR / \"features\" / \"dense\"\n",
    "dense_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "023ebe43-a421-4cb5-bd6f-da0876bee01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "dense_parts = []\n",
    "if INCLUDE_SENTIMENT and not sent_df.empty:\n",
    "    dense_parts.append(sent_df)\n",
    "if INCLUDE_POS and not pos_df.empty:\n",
    "    # keep normalized ratios only to be compact\n",
    "    pos_norm_cols = [c for c in pos_df.columns if c.endswith(\"_norm\")]\n",
    "    dense_parts.append(pos_df[pos_norm_cols])\n",
    "\n",
    "if len(dense_parts) > 0:\n",
    "    dense_features = pd.concat(dense_parts, axis=1).fillna(0.0)\n",
    "else:\n",
    "    dense_features = pd.DataFrame(index=all_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed2c137a-c613-4661-8a0d-56b80958a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dense features with shape: (122017, 22)\n"
     ]
    }
   ],
   "source": [
    "# Save dense features and column list\n",
    "dense_features.to_parquet(dense_dir / f\"dense_features_{ARTIFACT_VERSION}.parquet\")\n",
    "with open(dense_dir / f\"dense_feature_columns_{ARTIFACT_VERSION}.json\", \"w\") as f:\n",
    "    json.dump(list(dense_features.columns), f, indent=2)\n",
    "\n",
    "print(\"Saved dense features with shape:\", dense_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091e5c1d",
   "metadata": {},
   "source": [
    "### **Fit scaler on train dense features and save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b19907b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dense scaler.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import joblib\n",
    "\n",
    "# Directory where the scaler will be saved\n",
    "scaler_dir = dense_dir\n",
    "\n",
    "# Only scale if there are dense features\n",
    "if dense_features.shape[1] > 0:\n",
    "    # Load training indices to fit the scaler only on training data\n",
    "    train_idx = pd.read_csv(\n",
    "        ARTIFACT_DIR / \"splits\" / f\"train_idx_{ARTIFACT_VERSION}.csv\",\n",
    "        header=None\n",
    "    ).iloc[:, 0]\n",
    "\n",
    "    # Fit MaxAbsScaler on the training subset of dense features\n",
    "    scaler = MaxAbsScaler()\n",
    "    scaler.fit(dense_features.loc[train_idx])\n",
    "\n",
    "    # Save the fitted scaler for later use\n",
    "    joblib.dump(scaler, scaler_dir / f\"dense_scaler_{ARTIFACT_VERSION}.joblib\")\n",
    "    print(\"Saved dense scaler.\")\n",
    "else:\n",
    "    # If no dense features exist, skip scaling\n",
    "    print(\"No dense features to scale; skipping scaler.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7ac93",
   "metadata": {},
   "source": [
    "### **TF-IDF: fit on train, transform all splits, save matrices + vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b052e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load train, validation, and test indices\n",
    "train_idx = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"train_idx_{ARTIFACT_VERSION}.csv\", header=None).squeeze()\n",
    "valid_idx = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"valid_idx_{ARTIFACT_VERSION}.csv\", header=None).squeeze()\n",
    "test_idx  = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"test_idx_{ARTIFACT_VERSION}.csv\",  header=None).squeeze()\n",
    "\n",
    "# Directory for TF-IDF features\n",
    "tfidf_dir = ARTIFACT_DIR / \"features\" / \"tfidf\"\n",
    "tfidf_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f154fa3c-f423-489e-84a6-83320b9cfbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=TFIDF_NGRAM_RANGE,\n",
    "    min_df=TFIDF_MIN_DF,\n",
    "    max_df=TFIDF_MAX_DF,\n",
    "    max_features=TFIDF_MAX_FEATURES,\n",
    "    sublinear_tf=TFIDF_SUBLINEAR_TF,\n",
    "    norm=TFIDF_NORM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "193ebe38-b2b2-4bb1-8677-e405741c12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text for each split\n",
    "X_train_text = all_df.loc[train_idx, \"text\"].astype(str)\n",
    "X_valid_text = all_df.loc[valid_idx, \"text\"].astype(str)\n",
    "X_test_text  = all_df.loc[test_idx,  \"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "942c36de-84d6-4f8e-8174-0dfff4c9f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF-IDF on training data and transform all splits\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_valid_tfidf = tfidf.transform(X_valid_text)\n",
    "X_test_tfidf  = tfidf.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5c791ea-c1a3-4b4e-8865-8a7b7fd19238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorizer and TF-IDF matrices\n",
    "joblib.dump(tfidf, tfidf_dir / f\"tfidf_word_{ARTIFACT_VERSION}.joblib\")\n",
    "sparse.save_npz(tfidf_dir / f\"X_train_tfidf_{ARTIFACT_VERSION}.npz\", X_train_tfidf)\n",
    "sparse.save_npz(tfidf_dir / f\"X_valid_tfidf_{ARTIFACT_VERSION}.npz\", X_valid_tfidf)\n",
    "sparse.save_npz(tfidf_dir / f\"X_test_tfidf_{ARTIFACT_VERSION}.npz\",  X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76296bd4-f03a-47ab-b1f7-86f60624b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save split labels mapped to integer IDs\n",
    "labels_dir = ARTIFACT_DIR / \"labels\"\n",
    "all_labels = all_df[\"label\"].map(lambda x: label_to_id.get(x, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b79a717f-196b-4b4c-a9c2-81cf06e03b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels.loc[train_idx].to_csv(labels_dir / f\"y_train_{ARTIFACT_VERSION}.csv\", index=False, header=False)\n",
    "all_labels.loc[valid_idx].to_csv(labels_dir / f\"y_valid_{ARTIFACT_VERSION}.csv\", index=False, header=False)\n",
    "all_labels.loc[test_idx].to_csv(labels_dir / f\"y_test_{ARTIFACT_VERSION}.csv\",  index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "acc1f8ad-3c86-4368-bdfd-45058b6f0743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF saved: (109816, 67892) (6102, 67892) (6102, 67892)\n"
     ]
    }
   ],
   "source": [
    "# Confirm shapes of TF-IDF matrices\n",
    "print(\"TF-IDF saved:\",\n",
    "      X_train_tfidf.shape, X_valid_tfidf.shape, X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3bb0e",
   "metadata": {},
   "source": [
    "### **LSTM/RNN: tokenize, pad, save sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "178871c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 13:17:36.916503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Directory for sequence features\n",
    "seq_dir = ARTIFACT_DIR / \"features\" / \"sequences\"\n",
    "seq_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d091e2b-45a9-4da6-95df-4a4c0da07364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, validation, and test indices\n",
    "train_idx = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"train_idx_{ARTIFACT_VERSION}.csv\", header=None).squeeze()\n",
    "valid_idx = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"valid_idx_{ARTIFACT_VERSION}.csv\", header=None).squeeze()\n",
    "test_idx  = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"test_idx_{ARTIFACT_VERSION}.csv\",  header=None).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9674140d-9617-4dab-bbc6-2d2604bb4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text for each split\n",
    "texts_train = all_df.loc[train_idx, \"text\"].astype(str).tolist()\n",
    "texts_valid = all_df.loc[valid_idx, \"text\"].astype(str).tolist()\n",
    "texts_test  = all_df.loc[test_idx,  \"text\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4e93f06e-b89c-4f0c-89a3-7c046c5e8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit tokenizer on training text\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<UNK>\", lower=True)\n",
    "tokenizer.fit_on_texts(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dba94166-44e8-4f35-8f63-6ee1672685ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into integer sequences\n",
    "seq_train = tokenizer.texts_to_sequences(texts_train)\n",
    "seq_valid = tokenizer.texts_to_sequences(texts_valid)\n",
    "seq_test  = tokenizer.texts_to_sequences(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "640e4b49-9cb3-4f00-a637-b2ed43ad3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no max length is provided, choose it based on a percentile of training lengths\n",
    "if MAX_LEN is None:\n",
    "    lengths = [len(s) for s in seq_train]\n",
    "    MAX_LEN = int(np.percentile(lengths, MAX_LEN_PERCENTILE))\n",
    "    MAX_LEN = max(8, min(MAX_LEN, 256))  # enforce reasonable bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18cf1c2e-9aa0-4e4b-91c0-e85497448620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate sequences to the same length\n",
    "X_train_seq = pad_sequences(seq_train, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_valid_seq = pad_sequences(seq_valid, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_test_seq  = pad_sequences(seq_test,  maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37d17a82-d4fb-4f97-9486-e32836fa336d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artifacts/features/sequences/tokenizer_v1.joblib']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save padded sequences and tokenizer\n",
    "np.savez_compressed(seq_dir / f\"X_train_seq_{ARTIFACT_VERSION}.npz\", X_train_seq)\n",
    "np.savez_compressed(seq_dir / f\"X_valid_seq_{ARTIFACT_VERSION}.npz\", X_valid_seq)\n",
    "np.savez_compressed(seq_dir / f\"X_test_seq_{ARTIFACT_VERSION}.npz\",  X_test_seq)\n",
    "joblib.dump(tokenizer, seq_dir / f\"tokenizer_{ARTIFACT_VERSION}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c0b0a02-0982-4c02-aef9-7e7879d30e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata (vocabulary size and sequence length)\n",
    "with open(seq_dir / f\"sequence_meta_{ARTIFACT_VERSION}.txt\", \"w\") as f:\n",
    "    f.write(f\"VOCAB_SIZE={VOCAB_SIZE}\\n\")\n",
    "    f.write(f\"MAX_LEN={MAX_LEN}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ddf38fd8-504f-4765-a920-d8958968c46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sequences: (109816, 65) (6102, 65) (6102, 65)\n",
      "Tokenizer vocab size (limited): 40000\n"
     ]
    }
   ],
   "source": [
    "# Confirm results\n",
    "print(\"Saved sequences:\", X_train_seq.shape, X_valid_seq.shape, X_test_seq.shape)\n",
    "print(\"Tokenizer vocab size (limited):\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3a818",
   "metadata": {},
   "source": [
    "### **Transformers: tokenize with HuggingFace tokenizer and save arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e036b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Directory for transformer-based features\n",
    "tr_dir = ARTIFACT_DIR / \"features\" / \"transformers\"\n",
    "tr_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2151bdc0-b7e6-4812-aa8e-e846f55dbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Hugging Face model name used for reproducibility\n",
    "with open(tr_dir / f\"hf_model_name_{ARTIFACT_VERSION}.txt\", \"w\") as f:\n",
    "    f.write(HF_MODEL_NAME)\n",
    "\n",
    "# Load tokenizer from the specified Hugging Face model\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e5488e40-45ee-483c-a173-dbc623137a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, validation, and test indices\n",
    "train_idx = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"train_idx_{ARTIFACT_VERSION}.csv\", header=None).squeeze()\n",
    "valid_idx = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"valid_idx_{ARTIFACT_VERSION}.csv\", header=None).squeeze()\n",
    "test_idx  = pd.read_csv(ARTIFACT_DIR / \"splits\" / f\"test_idx_{ARTIFACT_VERSION}.csv\",  header=None).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83188f08-477a-42c3-9e23-c679e9ebd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text for each split\n",
    "texts_train = all_df.loc[train_idx, \"text\"].astype(str).tolist()\n",
    "texts_valid = all_df.loc[valid_idx, \"text\"].astype(str).tolist()\n",
    "texts_test  = all_df.loc[test_idx,  \"text\"].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f4f4ae0b-5afd-419e-b51c-afdc9d07bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to tokenize a batch of texts\n",
    "def tok_batch(texts):\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=TRANSFORMER_MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    return np.array(enc[\"input_ids\"], dtype=\"int32\"), np.array(enc[\"attention_mask\"], dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "246c953f-374c-483c-8a21-01c186691bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each split into input IDs and attention masks\n",
    "train_input_ids, train_attention_mask = tok_batch(texts_train)\n",
    "valid_input_ids, valid_attention_mask = tok_batch(texts_valid)\n",
    "test_input_ids,  test_attention_mask  = tok_batch(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "69eeef25-3233-4daa-a03f-355394b4564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save arrays for model training\n",
    "np.savez_compressed(tr_dir / f\"train_input_ids_{ARTIFACT_VERSION}.npz\", train_input_ids)\n",
    "np.savez_compressed(tr_dir / f\"valid_input_ids_{ARTIFACT_VERSION}.npz\", valid_input_ids)\n",
    "np.savez_compressed(tr_dir / f\"test_input_ids_{ARTIFACT_VERSION}.npz\",  test_input_ids)\n",
    "\n",
    "np.savez_compressed(tr_dir / f\"train_attention_mask_{ARTIFACT_VERSION}.npz\", train_attention_mask)\n",
    "np.savez_compressed(tr_dir / f\"valid_attention_mask_{ARTIFACT_VERSION}.npz\", valid_attention_mask)\n",
    "np.savez_compressed(tr_dir / f\"test_attention_mask_{ARTIFACT_VERSION}.npz\",  test_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fcf9932a-763f-4449-aca2-c1f668554c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer arrays:\n",
      "Train: (109816, 128) (109816, 128)\n",
      "Valid: (6102, 128) (6102, 128)\n",
      "Test : (6102, 128) (6102, 128)\n"
     ]
    }
   ],
   "source": [
    "# Confirm that arrays were saved and print their shapes\n",
    "print(\"Saved transformer arrays:\")\n",
    "print(\"Train:\", train_input_ids.shape, train_attention_mask.shape)\n",
    "print(\"Valid:\", valid_input_ids.shape, valid_attention_mask.shape)\n",
    "print(\"Test :\", test_input_ids.shape,  test_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce176c7d",
   "metadata": {},
   "source": [
    "### **Summary: artifact layout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69e49458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/\n",
      "  features/\n",
      "    transformers/\n",
      "      train_input_ids_v1.npz\n",
      "      test_input_ids_v1.npz\n",
      "      valid_attention_mask_v1.npz\n",
      "      valid_input_ids_v1.npz\n",
      "      test_attention_mask_v1.npz\n",
      "      hf_model_name_v1.txt\n",
      "      train_attention_mask_v1.npz\n",
      "    tfidf/\n",
      "      X_test_tfidf_v1.npz\n",
      "      X_train_tfidf_v1.npz\n",
      "      X_valid_tfidf_v1.npz\n",
      "      tfidf_word_v1.joblib\n",
      "    dense/\n",
      "      dense_scaler_v1.joblib\n",
      "      dense_feature_columns_v1.json\n",
      "      dense_features_v1.parquet\n",
      "    sequences/\n",
      "      X_test_seq_v1.npz\n",
      "      X_train_seq_v1.npz\n",
      "      X_valid_seq_v1.npz\n",
      "      sequence_meta_v1.txt\n",
      "      tokenizer_v1.joblib\n",
      "  labels/\n",
      "    y_test_v1.csv\n",
      "    y_valid_v1.csv\n",
      "    y_train_v1.csv\n",
      "    label_mapping_v1.json\n",
      "  splits/\n",
      "    train_idx_v1.csv\n",
      "    test_idx_v1.csv\n",
      "    valid_idx_v1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Walk through the artifact directory and print its structure\n",
    "for root, dirs, files in os.walk(ARTIFACT_DIR):\n",
    "    # Determine depth of the current folder for indentation\n",
    "    level = root.replace(str(ARTIFACT_DIR), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "\n",
    "    # Print current directory name\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "\n",
    "    # Print files inside the current directory with extra indentation\n",
    "    subindent = '  ' * (level + 1)\n",
    "    for f in files:\n",
    "        print(f\"{subindent}{f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
